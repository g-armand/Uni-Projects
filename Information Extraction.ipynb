{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6613052cd2b74b089e9447f1bd1d82f1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_0f7638a08eec4158a03314fd182e7731",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Downloading \u001b[3;36mhttps://storage.googleapis.com/allennlp-public-mo…\u001b[0m \u001b[38;2;249;38;114m━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m \u001b[35m 91%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[32m49.1/54.2\u001b[0m\n                                                                                    \u001b[32mMB       \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading <span style=\"color: #008080; text-decoration-color: #008080; font-style: italic\">https://storage.googleapis.com/allennlp-public-mo…</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 91%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span> <span style=\"color: #008000; text-decoration-color: #008000\">49.1/54.2</span>\n                                                                                    <span style=\"color: #008000; text-decoration-color: #008000\">MB       </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "0f7638a08eec4158a03314fd182e7731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ8Q9yuOD6a8",
        "outputId": "d59323f0-23c2-441a-a647-d758c3b892ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def preprocess(filepath):\n",
        "\n",
        "  preprocessed = []\n",
        "\n",
        "  with open(filepath, 'r') as file:\n",
        "    text = ' '.join(file.readlines())\n",
        "    text = re.split(r'\\s?#?\\d+.+.\\n\\s+', text)\n",
        "    text = [line.split('\\n') for line in text]\n",
        "\n",
        "    for sent in text:\n",
        "      new_sent = []\n",
        "      for line in sent:\n",
        "        line = re.sub('<TAB>', ',', line)\n",
        "        line = line.split(' , ')\n",
        "\n",
        "        if line != [''] and line != [' ']:\n",
        "          new_sent.append(line)\n",
        "\n",
        "      preprocessed.append(new_sent)\n",
        "\n",
        "    #TODO peut-etre supprimer '(speculative)' flag car il est pas toujours respecté\n",
        "\n",
        "  return preprocessed\n",
        "\n",
        "def pad(annotations1, annotations2):\n",
        "\n",
        "    #j'aurais pu faure una matrice de zero et la remplir mais j'ai eu l'idée trop tard\n",
        "\n",
        "    padded1 = []\n",
        "    padded2 = []\n",
        "\n",
        "    for s1, s2 in zip(annotations1, annotations2):\n",
        "\n",
        "        max_len = max(len(s1), len(s2))\n",
        "        for i in range (max_len):\n",
        "            sent1=[]\n",
        "            sent2=[]\n",
        "            if i>=len(s1):\n",
        "                sent1=[\"\" for _ in range (len(s2[i]))]\n",
        "            else:\n",
        "                sent1=s1[i]\n",
        "\n",
        "            if i>=len(s2):\n",
        "                sent2=[\"\" for _ in range (len(s1[i]))]\n",
        "            else:\n",
        "                sent2=s2[i]\n",
        "\n",
        "            if i < len(s1) and  i < len(s2) and len(sent1) < len(sent2):\n",
        "                while len(sent1) < len(sent2):\n",
        "                    sent1.append('')\n",
        "\n",
        "            if i < len(s1) and  i < len(s2) and len(sent2) < len(sent1):\n",
        "                while len(sent2) < len(sent1):\n",
        "                    sent2.append('')\n",
        "\n",
        "            padded1.append(sent1)\n",
        "            padded2.append(sent2)\n",
        "\n",
        "\n",
        "\n",
        "    return padded1, padded2"
      ],
      "metadata": {
        "id": "0EOOpmksQsa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann1=[[], [['He / (Chomsky)', 'created', 'the universal grammar theory'], [' He / (Chomsky)', 'created', 'the generative grammar theory'], [' He / (Chomsky)', 'created', 'the Chomsky hierarchy'], [' He / (Chomsky)', 'created', 'the the minimalist program'], [' He / (Chomsky)', 'created', 'the universal grammar theory', 'the generative grammar'], [' theory', 'the Chomsky hierarchy and', 'the minimalist program']], [['was particularly critical of the work of B. F. Skinner .'], [' Chomsky', 'played a pivotal role in', 'the decline of', 'linguistic behaviorism'], [' Chomsky', 'was particularly critical of', 'the work of', 'B. F. Skinner .'], [' Chomsky', 'criticised', 'the work of', 'B. F. Skinner .']], [['act of American imperialism', 'in']], [['anti-war essay \" The Responsibility of Intellectuals \" .'], [' [Chomsky]', 'was', 'an opponent of', 'U.S. involvement in', 'the Vietnam War'], [\" _comment: est-ce qu'on garde l'adjectif dans les cas pareils? Genre outspoken_\"], [' He / (Chomsky)', 'saw', 'the Vietnam War', 'as an act of', 'American imperialism'], [' Chomsky', 'rose to national attention for', 'his / (Chomsky\\'s) anti-war essay \" The Responsibility of Intellectuals \"', 'in']], [['[Chomsky]', '[wrote]', '\" The Responsibility of Intellectuals \"']], [[\"activism and placed on President Richard Nixon 's list of political opponents .\"], [' He / (Chomsky)', 'was arrested for', \"his / (Chomsky's) activism\", 'multiple times'], [' He / (Chomsky)', 'was placed on', \"President Richard Nixon 's list of\", 'political opponents'], [' [Richard Nixon]', '[had] political opponents'], [' [Richard Nixon]', '[was a] president']], [['involved in the linguistics wars .'], [' He / (Chomsky)', 'became involved in', 'the linguistics wars'], [' He / (Chomsky)', '[has been] expanding', \"his / (Chomsky's) work in\", 'linguistics', 'over subsequent decades / (after']], [], [['model of media criticism in Manufacturing Consent', 'and worked to expose the'], [' Indonesian occupation of East Timor .'], [' Chomsky', 'collaborated with', 'Edward S. Herman'], [' Chomsky', 'articulated', 'the propaganda model of', 'media criticism', 'in Manufacturing Consent'], [\" _comment: ajouter 'later' c'est une idée à la con c ça?_\"], [' Chomsky', 'worked to expose', 'the Indonesian occupation of', 'East Timor']], [['denial', 'generated significant controversy in the Faurisson affair of the']], [[\"His / (Chomsky's) defense\", 'of unconditional freedom of', 'speech', 'generated significant controversy in', 'the Faurisson affair', 'of the']], [['[Chomsky]', '[defended]', 'freedom of speech'], [\" His / (Chomsky's) defense\", 'included', 'Holocaust denial '], [' _comment: peut-être ça a pas de sens les gars je suis défonsé je sais meme pas écrire défonsé_']], [[\"Chomsky 's commentary on\", 'the Cambodian genocide', 'generated controversy'], [' [Chomsky]', '[commented] on', 'the Cambodian genocide']], [['activism', 'including opposing the']], [['movement .'], [' [Chomsky]', '[retired] from', 'teaching', 'at MIT'], [' He / (Chomsky)', 'has continued', \"his / (Chomsky's) vocal political activism\"], [' He / (Chomsky)', '[opposed] the invasion <TAB >of Iraq', 'in']], [['He / (Chomsky)', '[supported]', 'the Occupy movement']], [['Chomsky', 'began teaching at', 'the University of Arizona', 'in']], []]\n",
        "ann2=[[], [['He /(Avram Noam Chomsky)', 'created  or co-created', 'the universal grammar theory'], [' He /(Avram Noam Chomsky)', 'created  or co-created', 'the generative grammar theory'], [' He /(Avram Noam Chomsky)', 'created  or co-created', 'the Chomsky hierarchy '], [' He /(Avram Noam Chomsky)', 'created  or co-created', 'the minimalist program']], [['Chomsky/ (Avram Noam Chomsky)', 'played a pivotal role', 'in the decline of linguistic behaviorism '], [' Chomsky/ (Avram Noam Chomsky)', 'was crticial of', 'the work of B. F. Skinner /(Burrhus Frederic Skinner)']], [['[Avram Noam Chomsky]', '[was] an opponent of ', 'U.S. involvement in the Vietnam war'], [' [Avram Noam Chomsky]', '[was] saw', 'U.S. involvement in the Vietnam war', 'as an act of American imperialism'], [' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\" '], [' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\"', 'in']], [['Chomsky/ (Avram Noam Chomsky)', 'rose to national attention', 'for his anti-war essay \"The Responsibility of Intellectuals\" '], [' \"The Responsibility of Intellectuals\"', '[is]', '[an] anti-war essay']], [['[Avram Noam Chomsky]', '[was] associated with', 'the New Left'], [' He/ (Avram Noam Chomsky)', 'was arrested '], [' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times'], [' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times', 'for his activism '], [' He/ (Avram Noam Chomsky)', 'was placed', \"on President Richard Nixon 's list of political opponents\"], [' Richard Nixon', '[was]', 'president'], [' Richard Nixon', '[had]', 'political opponents']], [['[Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars'], [' [Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars', 'while expanding his work in linguistics'], [' He/ (Avram Noam Chomsky)', '[expanded] his work', 'in linguistics over [several] decades']], [['Chomsky/ (Avram Noam Chomsky)', '[collaborated] with', 'Edward S. Herman'], [' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism '], [' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism', 'in Manufacturing Consent'], [' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'worked to expose', 'the Indonesian occupation of East Timor'], [' Indonesia', '[occupied]', 'East Timor']], [['[Avram Noam Chomsky]', '[defended]', 'unconditioanl freedom of speech'], [' unconditioanl freedom of speech', '[includes]', '[the] Holocaust denial'], [\" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy '], [\" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy', 'in the Faurisson affair of the']], [], [[\"Chomsky's/ (Avram Noam Chomsky's) commentary on the Cambodian genocide\", 'generated controversy']], [['he/ (Avram Noam Chomsky)', '[retired] from', 'active teaching at MIT'], [' he/ (Avram Noam Chomsky)', '[practiced]', 'vocal political activism '], [' he/ (Avram Noam Chomsky)', 'continued', 'his vocal political activism', 'after  retiring from active teaching at MIT'], [\" his/(Avram Noam Chomsky's) activism\", '[included]', 'opposing the']], [[\"his/(Avram Noam Chomsky's) activism\", '[supported]', 'the Occupy movement']], [['Chomsky/ (Avram Noam Chomsky)', '[teached]', 'at the University of Arizona'], [' Chomsky/ (Avram Noam Chomsky)', 'began teaching', 'at the University of Arizona in 2017']]]\n"
      ],
      "metadata": {
        "id": "GpVHRgkKdj_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "\n",
        "x, y = pad(ann1, ann2)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnYIY4XbYcAb",
        "outputId": "6b3b29fc-f6a8-49b4-90ae-9465cd000915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['He / (Chomsky)', 'created', 'the universal grammar theory'], [' He / (Chomsky)', 'created', 'the generative grammar theory'], [' He / (Chomsky)', 'created', 'the Chomsky hierarchy'], [' He / (Chomsky)', 'created', 'the the minimalist program'], [' He / (Chomsky)', 'created', 'the universal grammar theory', 'the generative grammar'], [' theory', 'the Chomsky hierarchy and', 'the minimalist program'], ['was particularly critical of the work of B. F. Skinner .', '', ''], [' Chomsky', 'played a pivotal role in', 'the decline of', 'linguistic behaviorism'], [' Chomsky', 'was particularly critical of', 'the work of', 'B. F. Skinner .'], [' Chomsky', 'criticised', 'the work of', 'B. F. Skinner .'], ['act of American imperialism', 'in', ''], ['', '', '', ''], ['', '', ''], ['', '', '', ''], ['anti-war essay \" The Responsibility of Intellectuals \" .', '', ''], [' [Chomsky]', 'was', 'an opponent of', 'U.S. involvement in', 'the Vietnam War'], [\" _comment: est-ce qu'on garde l'adjectif dans les cas pareils? Genre outspoken_\"], [' He / (Chomsky)', 'saw', 'the Vietnam War', 'as an act of', 'American imperialism'], [' Chomsky', 'rose to national attention for', 'his / (Chomsky\\'s) anti-war essay \" The Responsibility of Intellectuals \"', 'in'], ['[Chomsky]', '[wrote]', '\" The Responsibility of Intellectuals \"'], ['', ''], ['', '', ''], ['', '', '', ''], ['', '', ''], ['', '', ''], ['', '', ''], [\"activism and placed on President Richard Nixon 's list of political opponents .\", '', ''], [' He / (Chomsky)', 'was arrested for', \"his / (Chomsky's) activism\", 'multiple times'], [' He / (Chomsky)', 'was placed on', \"President Richard Nixon 's list of\", 'political opponents'], [' [Richard Nixon]', '[had] political opponents'], [' [Richard Nixon]', '[was a] president'], ['involved in the linguistics wars .', '', ''], [' He / (Chomsky)', 'became involved in', 'the linguistics wars'], [' He / (Chomsky)', '[has been] expanding', \"his / (Chomsky's) work in\", 'linguistics', 'over subsequent decades / (after'], ['', '', ''], ['', '', ''], ['', '', ''], ['', '', ''], ['', '', ''], ['', '', '', ''], ['model of media criticism in Manufacturing Consent', 'and worked to expose the'], [' Indonesian occupation of East Timor .'], [' Chomsky', 'collaborated with', 'Edward S. Herman'], [' Chomsky', 'articulated', 'the propaganda model of', 'media criticism', 'in Manufacturing Consent'], [\" _comment: ajouter 'later' c'est une idée à la con c ça?_\"], [' Chomsky', 'worked to expose', 'the Indonesian occupation of', 'East Timor'], ['denial', 'generated significant controversy in the Faurisson affair of the'], [\"His / (Chomsky's) defense\", 'of unconditional freedom of', 'speech', 'generated significant controversy in', 'the Faurisson affair', 'of the'], ['', '', ''], ['', '', '', ''], ['', '', ''], ['[Chomsky]', '[defended]', 'freedom of speech'], [\" His / (Chomsky's) defense\", 'included', 'Holocaust denial '], [' _comment: peut-être ça a pas de sens les gars je suis défonsé je sais meme pas écrire défonsé_'], [\"Chomsky 's commentary on\", 'the Cambodian genocide', 'generated controversy'], [' [Chomsky]', '[commented] on', 'the Cambodian genocide']]\n",
            "[['He /(Avram Noam Chomsky)', 'created  or co-created', 'the universal grammar theory'], [' He /(Avram Noam Chomsky)', 'created  or co-created', 'the generative grammar theory'], [' He /(Avram Noam Chomsky)', 'created  or co-created', 'the Chomsky hierarchy '], [' He /(Avram Noam Chomsky)', 'created  or co-created', 'the minimalist program'], ['', '', '', ''], ['', '', ''], ['Chomsky/ (Avram Noam Chomsky)', 'played a pivotal role', 'in the decline of linguistic behaviorism '], [' Chomsky/ (Avram Noam Chomsky)', 'was crticial of', 'the work of B. F. Skinner /(Burrhus Frederic Skinner)', ''], ['', '', '', ''], ['', '', '', ''], ['[Avram Noam Chomsky]', '[was] an opponent of ', 'U.S. involvement in the Vietnam war'], [' [Avram Noam Chomsky]', '[was] saw', 'U.S. involvement in the Vietnam war', 'as an act of American imperialism'], [' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\" '], [' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\"', 'in'], ['Chomsky/ (Avram Noam Chomsky)', 'rose to national attention', 'for his anti-war essay \"The Responsibility of Intellectuals\" '], [' \"The Responsibility of Intellectuals\"', '[is]', '[an] anti-war essay', '', ''], [''], ['', '', '', '', ''], ['', '', '', ''], ['[Avram Noam Chomsky]', '[was] associated with', 'the New Left'], [' He/ (Avram Noam Chomsky)', 'was arrested '], [' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times'], [' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times', 'for his activism '], [' He/ (Avram Noam Chomsky)', 'was placed', \"on President Richard Nixon 's list of political opponents\"], [' Richard Nixon', '[was]', 'president'], [' Richard Nixon', '[had]', 'political opponents'], ['[Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars'], [' [Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars', 'while expanding his work in linguistics'], [' He/ (Avram Noam Chomsky)', '[expanded] his work', 'in linguistics over [several] decades', ''], ['', ''], ['', ''], ['Chomsky/ (Avram Noam Chomsky)', '[collaborated] with', 'Edward S. Herman'], [' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism '], [' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism', 'in Manufacturing Consent', ''], [' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'worked to expose', 'the Indonesian occupation of East Timor'], [' Indonesia', '[occupied]', 'East Timor'], ['[Avram Noam Chomsky]', '[defended]', 'unconditioanl freedom of speech'], [' unconditioanl freedom of speech', '[includes]', '[the] Holocaust denial'], [\" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy '], [\" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy', 'in the Faurisson affair of the'], ['', ''], [''], ['', '', ''], ['', '', '', '', ''], [''], ['', '', '', ''], [\"Chomsky's/ (Avram Noam Chomsky's) commentary on the Cambodian genocide\", 'generated controversy'], ['he/ (Avram Noam Chomsky)', '[retired] from', 'active teaching at MIT', '', '', ''], [' he/ (Avram Noam Chomsky)', '[practiced]', 'vocal political activism '], [' he/ (Avram Noam Chomsky)', 'continued', 'his vocal political activism', 'after  retiring from active teaching at MIT'], [\" his/(Avram Noam Chomsky's) activism\", '[included]', 'opposing the'], [\"his/(Avram Noam Chomsky's) activism\", '[supported]', 'the Occupy movement'], ['', '', ''], [''], ['Chomsky/ (Avram Noam Chomsky)', '[teached]', 'at the University of Arizona'], [' Chomsky/ (Avram Noam Chomsky)', 'began teaching', 'at the University of Arizona in 2017']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculer sur toute l'annotation\n",
        "x = list(chain.from_iterable(x))\n",
        "y = list(chain.from_iterable(y))\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(\"{:.2f}\".format(sklearn.metrics.cohen_kappa_score(x, y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5KGwgAz981M",
        "outputId": "07fa5da8-03ab-427a-f854-97efec2f636f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He / (Chomsky)', 'created', 'the universal grammar theory', ' He / (Chomsky)', 'created', 'the generative grammar theory', ' He / (Chomsky)', 'created', 'the Chomsky hierarchy', ' He / (Chomsky)', 'created', 'the the minimalist program', ' He / (Chomsky)', 'created', 'the universal grammar theory', 'the generative grammar', ' theory', 'the Chomsky hierarchy and', 'the minimalist program', 'was particularly critical of the work of B. F. Skinner .', '', '', ' Chomsky', 'played a pivotal role in', 'the decline of', 'linguistic behaviorism', ' Chomsky', 'was particularly critical of', 'the work of', 'B. F. Skinner .', ' Chomsky', 'criticised', 'the work of', 'B. F. Skinner .', 'act of American imperialism', 'in', '', '', '', '', '', '', '', '', '', '', '', '', 'anti-war essay \" The Responsibility of Intellectuals \" .', '', '', ' [Chomsky]', 'was', 'an opponent of', 'U.S. involvement in', 'the Vietnam War', \" _comment: est-ce qu'on garde l'adjectif dans les cas pareils? Genre outspoken_\", ' He / (Chomsky)', 'saw', 'the Vietnam War', 'as an act of', 'American imperialism', ' Chomsky', 'rose to national attention for', 'his / (Chomsky\\'s) anti-war essay \" The Responsibility of Intellectuals \"', 'in', '[Chomsky]', '[wrote]', '\" The Responsibility of Intellectuals \"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"activism and placed on President Richard Nixon 's list of political opponents .\", '', '', ' He / (Chomsky)', 'was arrested for', \"his / (Chomsky's) activism\", 'multiple times', ' He / (Chomsky)', 'was placed on', \"President Richard Nixon 's list of\", 'political opponents', ' [Richard Nixon]', '[had] political opponents', ' [Richard Nixon]', '[was a] president', 'involved in the linguistics wars .', '', '', ' He / (Chomsky)', 'became involved in', 'the linguistics wars', ' He / (Chomsky)', '[has been] expanding', \"his / (Chomsky's) work in\", 'linguistics', 'over subsequent decades / (after', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'model of media criticism in Manufacturing Consent', 'and worked to expose the', ' Indonesian occupation of East Timor .', ' Chomsky', 'collaborated with', 'Edward S. Herman', ' Chomsky', 'articulated', 'the propaganda model of', 'media criticism', 'in Manufacturing Consent', \" _comment: ajouter 'later' c'est une idée à la con c ça?_\", ' Chomsky', 'worked to expose', 'the Indonesian occupation of', 'East Timor', 'denial', 'generated significant controversy in the Faurisson affair of the', \"His / (Chomsky's) defense\", 'of unconditional freedom of', 'speech', 'generated significant controversy in', 'the Faurisson affair', 'of the', '', '', '', '', '', '', '', '', '', '', '[Chomsky]', '[defended]', 'freedom of speech', \" His / (Chomsky's) defense\", 'included', 'Holocaust denial ', ' _comment: peut-être ça a pas de sens les gars je suis défonsé je sais meme pas écrire défonsé_', \"Chomsky 's commentary on\", 'the Cambodian genocide', 'generated controversy', ' [Chomsky]', '[commented] on', 'the Cambodian genocide']\n",
            "['He /(Avram Noam Chomsky)', 'created  or co-created', 'the universal grammar theory', ' He /(Avram Noam Chomsky)', 'created  or co-created', 'the generative grammar theory', ' He /(Avram Noam Chomsky)', 'created  or co-created', 'the Chomsky hierarchy ', ' He /(Avram Noam Chomsky)', 'created  or co-created', 'the minimalist program', '', '', '', '', '', '', '', 'Chomsky/ (Avram Noam Chomsky)', 'played a pivotal role', 'in the decline of linguistic behaviorism ', ' Chomsky/ (Avram Noam Chomsky)', 'was crticial of', 'the work of B. F. Skinner /(Burrhus Frederic Skinner)', '', '', '', '', '', '', '', '', '', '[Avram Noam Chomsky]', '[was] an opponent of ', 'U.S. involvement in the Vietnam war', ' [Avram Noam Chomsky]', '[was] saw', 'U.S. involvement in the Vietnam war', 'as an act of American imperialism', ' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\" ', ' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\"', 'in', 'Chomsky/ (Avram Noam Chomsky)', 'rose to national attention', 'for his anti-war essay \"The Responsibility of Intellectuals\" ', ' \"The Responsibility of Intellectuals\"', '[is]', '[an] anti-war essay', '', '', '', '', '', '', '', '', '', '', '', '', '[Avram Noam Chomsky]', '[was] associated with', 'the New Left', ' He/ (Avram Noam Chomsky)', 'was arrested ', ' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times', ' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times', 'for his activism ', ' He/ (Avram Noam Chomsky)', 'was placed', \"on President Richard Nixon 's list of political opponents\", ' Richard Nixon', '[was]', 'president', ' Richard Nixon', '[had]', 'political opponents', '[Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars', ' [Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars', 'while expanding his work in linguistics', ' He/ (Avram Noam Chomsky)', '[expanded] his work', 'in linguistics over [several] decades', '', '', '', '', '', 'Chomsky/ (Avram Noam Chomsky)', '[collaborated] with', 'Edward S. Herman', ' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism ', ' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism', 'in Manufacturing Consent', '', ' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'worked to expose', 'the Indonesian occupation of East Timor', ' Indonesia', '[occupied]', 'East Timor', '[Avram Noam Chomsky]', '[defended]', 'unconditioanl freedom of speech', ' unconditioanl freedom of speech', '[includes]', '[the] Holocaust denial', \" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy ', \" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy', 'in the Faurisson affair of the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"Chomsky's/ (Avram Noam Chomsky's) commentary on the Cambodian genocide\", 'generated controversy', 'he/ (Avram Noam Chomsky)', '[retired] from', 'active teaching at MIT', '', '', '', ' he/ (Avram Noam Chomsky)', '[practiced]', 'vocal political activism ', ' he/ (Avram Noam Chomsky)', 'continued', 'his vocal political activism', 'after  retiring from active teaching at MIT', \" his/(Avram Noam Chomsky's) activism\", '[included]', 'opposing the', \"his/(Avram Noam Chomsky's) activism\", '[supported]', 'the Occupy movement', '', '', '', '', 'Chomsky/ (Avram Noam Chomsky)', '[teached]', 'at the University of Arizona', ' Chomsky/ (Avram Noam Chomsky)', 'began teaching', 'at the University of Arizona in 2017']\n",
            "-0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculer sentence par sentence et trouver la moyenne\n",
        "import sklearn.metrics\n",
        "\n",
        "x, y = pad(ann1, ann2)\n",
        "scores = []\n",
        "\n",
        "for sent1, sent2 in zip(x,y):\n",
        "  score = sklearn.metrics.cohen_kappa_score(sent1, sent2)\n",
        "  scores.append(score)\n",
        "\n",
        "  # decommenter pour voir les phrases et les scores precises\n",
        "  # print(sent1, sent2)\n",
        "  # print(\"{:.2f}\".format(score))\n",
        "\n",
        "print('average kappa score: ', \"{:.3f}\".format(sum(scores)/len(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfWDaKo2QIJH",
        "outputId": "cc19a853-15b9-4845-8193-16816d891263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average kappa score:  0.009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ici in pourra calculer le score entre l'annotation\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# f1 = f1_score(y_true= ##, y_pred=##)\n",
        "print(f1)"
      ],
      "metadata": {
        "id": "9BjvFGHVEnDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ce truc est horrible les amis il veut un format chelou\n",
        "# il y a un exemple mais bof https://gist.github.com/cbuntain/9dd7e42d5d8ab34609162410e06f3270\n",
        "\n",
        "# from nltk.metrics.agreement import AnnotationTask\n",
        "# t = AnnotationTask(data=[x, y])"
      ],
      "metadata": {
        "id": "Qdr2tacJAP4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "id": "2ISUM-83DtQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -v stanford_openie\n"
      ],
      "metadata": {
        "id": "gGqLv5TTDhlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openie import StanfordOpenIE\n",
        "\n",
        "# https://stanfordnlp.github.io/CoreNLP/openie.html#api\n",
        "# Default value of openie.affinity_probability_cap was 1/3.\n",
        "properties = {\n",
        "    'openie.affinity_probability_cap': 1 / 3,\n",
        "}\n",
        "\n",
        "with StanfordOpenIE(properties=properties) as client:\n",
        "    text = 'Barack Obama was born in Hawaii. Richard Manning wrote this sentence.'\n",
        "    print('Text: %s.' % text)\n",
        "    for triple in client.annotate(text):\n",
        "        print('|-', triple)\n",
        "\n",
        "    graph_image = 'graph.png'\n",
        "    client.generate_graphviz_graph(text, graph_image)\n",
        "    print('Graph generated: %s.' % graph_image)\n",
        "\n",
        "    with open('corpus/pg6130.txt', encoding='utf8') as r:\n",
        "        corpus = r.read().replace('\\n', ' ').replace('\\r', '')\n",
        "\n",
        "    triples_corpus = client.annotate(corpus[0:5000])\n",
        "    print('Corpus: %s [...].' % corpus[0:80])\n",
        "    print('Found %s triples in the corpus.' % len(triples_corpus))\n",
        "    for triple in triples_corpus[:3]:\n",
        "        print('|-', triple)\n",
        "    print('[...]')"
      ],
      "metadata": {
        "id": "0y2Kuc_vDZcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "from nltk.metrics import agreement\n",
        "\n",
        "def edit_distance_no_zero (sent_a, sent_b):\n",
        "    dist = edit_distance(sent_a, sent_b)  / max(len(sent_a), len(sent_b)) + 0.0001\n",
        "    return dist\n",
        "\n",
        "scorer = agreement.AnnotationTask(data = [[\"Alina\", 1, 'abcd'], [\"Eleonore\", 1, 'abc'], [\"Alina\", 1, 'abc'], [\"Eleonore\", 1, 'abcd']] ,distance=edit_distance_no_zero)"
      ],
      "metadata": {
        "id": "o9UgHQfrFa1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer.kappa()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAymBSvhGVGr",
        "outputId": "acc36a62-5673-41da-e47d-00e0eb6c0528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2501"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "a = [\"the Cambodian genocide\" ]\n",
        "b = [\"at the University of Arizona in 2017\" ]\n",
        "\n",
        "\n",
        "dist = sklearn.metrics.cohen_kappa_score(a, b, weights='quadratic', sample_weight = [1-edit_distance_no_zero(tupa, tupb) for tupa, tupb in zip(a, b)])\n",
        "print(dist)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(\"{:.2f}\".format(sklearn.metrics.cohen_kappa_score(x, y,  weights='linear', sample_weight = [1-edit_distance_no_zero(tupa, tupb) for tupa, tupb in zip(x, y)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pd4D-NBQctk",
        "outputId": "16502d27-2f63-4fe6-fd8b-a3f7c973954d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "['He / (Chomsky)', 'created', 'the universal grammar theory', ' He / (Chomsky)', 'created', 'the generative grammar theory', ' He / (Chomsky)', 'created', 'the Chomsky hierarchy', ' He / (Chomsky)', 'created', 'the the minimalist program', ' He / (Chomsky)', 'created', 'the universal grammar theory', 'the generative grammar', ' theory', 'the Chomsky hierarchy and', 'the minimalist program', 'was particularly critical of the work of B. F. Skinner .', '', '', ' Chomsky', 'played a pivotal role in', 'the decline of', 'linguistic behaviorism', ' Chomsky', 'was particularly critical of', 'the work of', 'B. F. Skinner .', ' Chomsky', 'criticised', 'the work of', 'B. F. Skinner .', 'act of American imperialism', 'in', '', '', '', '', '', '', '', '', '', '', '', '', 'anti-war essay \" The Responsibility of Intellectuals \" .', '', '', ' [Chomsky]', 'was', 'an opponent of', 'U.S. involvement in', 'the Vietnam War', \" _comment: est-ce qu'on garde l'adjectif dans les cas pareils? Genre outspoken_\", ' He / (Chomsky)', 'saw', 'the Vietnam War', 'as an act of', 'American imperialism', ' Chomsky', 'rose to national attention for', 'his / (Chomsky\\'s) anti-war essay \" The Responsibility of Intellectuals \"', 'in', '[Chomsky]', '[wrote]', '\" The Responsibility of Intellectuals \"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"activism and placed on President Richard Nixon 's list of political opponents .\", '', '', ' He / (Chomsky)', 'was arrested for', \"his / (Chomsky's) activism\", 'multiple times', ' He / (Chomsky)', 'was placed on', \"President Richard Nixon 's list of\", 'political opponents', ' [Richard Nixon]', '[had] political opponents', ' [Richard Nixon]', '[was a] president', 'involved in the linguistics wars .', '', '', ' He / (Chomsky)', 'became involved in', 'the linguistics wars', ' He / (Chomsky)', '[has been] expanding', \"his / (Chomsky's) work in\", 'linguistics', 'over subsequent decades / (after', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'model of media criticism in Manufacturing Consent', 'and worked to expose the', ' Indonesian occupation of East Timor .', ' Chomsky', 'collaborated with', 'Edward S. Herman', ' Chomsky', 'articulated', 'the propaganda model of', 'media criticism', 'in Manufacturing Consent', \" _comment: ajouter 'later' c'est une idée à la con c ça?_\", ' Chomsky', 'worked to expose', 'the Indonesian occupation of', 'East Timor', 'denial', 'generated significant controversy in the Faurisson affair of the', \"His / (Chomsky's) defense\", 'of unconditional freedom of', 'speech', 'generated significant controversy in', 'the Faurisson affair', 'of the', '', '', '', '', '', '', '', '', '', '', '[Chomsky]', '[defended]', 'freedom of speech', \" His / (Chomsky's) defense\", 'included', 'Holocaust denial ', ' _comment: peut-être ça a pas de sens les gars je suis défonsé je sais meme pas écrire défonsé_', \"Chomsky 's commentary on\", 'the Cambodian genocide', 'generated controversy', ' [Chomsky]', '[commented] on', 'the Cambodian genocide']\n",
            "['He /(Avram Noam Chomsky)', 'created  or co-created', 'the universal grammar theory', ' He /(Avram Noam Chomsky)', 'created  or co-created', 'the generative grammar theory', ' He /(Avram Noam Chomsky)', 'created  or co-created', 'the Chomsky hierarchy ', ' He /(Avram Noam Chomsky)', 'created  or co-created', 'the minimalist program', '', '', '', '', '', '', '', 'Chomsky/ (Avram Noam Chomsky)', 'played a pivotal role', 'in the decline of linguistic behaviorism ', ' Chomsky/ (Avram Noam Chomsky)', 'was crticial of', 'the work of B. F. Skinner /(Burrhus Frederic Skinner)', '', '', '', '', '', '', '', '', '', '[Avram Noam Chomsky]', '[was] an opponent of ', 'U.S. involvement in the Vietnam war', ' [Avram Noam Chomsky]', '[was] saw', 'U.S. involvement in the Vietnam war', 'as an act of American imperialism', ' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\" ', ' Chomsky/ (Avram Noam Chomsky)', '[wrote]', '\"The Responsibility of Intellectuals\"', 'in', 'Chomsky/ (Avram Noam Chomsky)', 'rose to national attention', 'for his anti-war essay \"The Responsibility of Intellectuals\" ', ' \"The Responsibility of Intellectuals\"', '[is]', '[an] anti-war essay', '', '', '', '', '', '', '', '', '', '', '', '', '[Avram Noam Chomsky]', '[was] associated with', 'the New Left', ' He/ (Avram Noam Chomsky)', 'was arrested ', ' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times', ' He/ (Avram Noam Chomsky)', 'was arrested', 'multiple times', 'for his activism ', ' He/ (Avram Noam Chomsky)', 'was placed', \"on President Richard Nixon 's list of political opponents\", ' Richard Nixon', '[was]', 'president', ' Richard Nixon', '[had]', 'political opponents', '[Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars', ' [Avram Noam Chomsky]', '[was] involved in', 'the linguistic wars', 'while expanding his work in linguistics', ' He/ (Avram Noam Chomsky)', '[expanded] his work', 'in linguistics over [several] decades', '', '', '', '', '', 'Chomsky/ (Avram Noam Chomsky)', '[collaborated] with', 'Edward S. Herman', ' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism ', ' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'articulated', 'the propaganda model of media criticism', 'in Manufacturing Consent', '', ' Chomsky/ (Avram Noam Chomsky) [and] Edward S. Herman', 'worked to expose', 'the Indonesian occupation of East Timor', ' Indonesia', '[occupied]', 'East Timor', '[Avram Noam Chomsky]', '[defended]', 'unconditioanl freedom of speech', ' unconditioanl freedom of speech', '[includes]', '[the] Holocaust denial', \" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy ', \" His/(Avram Noam Chomsky's) defense of unconditional freedom of speech\", 'generated', 'controversy', 'in the Faurisson affair of the', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', \"Chomsky's/ (Avram Noam Chomsky's) commentary on the Cambodian genocide\", 'generated controversy', 'he/ (Avram Noam Chomsky)', '[retired] from', 'active teaching at MIT', '', '', '', ' he/ (Avram Noam Chomsky)', '[practiced]', 'vocal political activism ', ' he/ (Avram Noam Chomsky)', 'continued', 'his vocal political activism', 'after  retiring from active teaching at MIT', \" his/(Avram Noam Chomsky's) activism\", '[included]', 'opposing the', \"his/(Avram Noam Chomsky's) activism\", '[supported]', 'the Occupy movement', '', '', '', '', 'Chomsky/ (Avram Noam Chomsky)', '[teached]', 'at the University of Arizona', ' Chomsky/ (Avram Noam Chomsky)', 'began teaching', 'at the University of Arizona in 2017']\n",
            "0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[1-edit_distance_no_zero(tupa, tupb) for tupa, tupb in zip(a, b)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcKdCHMoSwrn",
        "outputId": "46043b1f-d389-423c-9c9a-e7880a59349f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9999, 0.9544454545454546, 0.9999]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools\n",
        "!pip install pyiextract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UcDfItEHkW91",
        "outputId": "c5197905-25c4-4372-b6a8-bd70eed0a18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.3.2 setuptools-69.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyiextract\n",
            "  Downloading pyiextract-0.0.4.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting textacy==0.12.0 (from pyiextract)\n",
            "  Downloading textacy-0.12.0-py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.4/208.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting allennlp==2.10.0 (from pyiextract)\n",
            "  Downloading allennlp-2.10.0-py3-none-any.whl (729 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.8/729.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting allennlp-models==2.10.0 (from pyiextract)\n",
            "  Downloading allennlp_models-2.10.0-py3-none-any.whl (464 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.5/464.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers==4.20.1 in /usr/local/lib/python3.10/dist-packages (from pyiextract) (4.20.1)\n",
            "Requirement already satisfied: textblob==0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyiextract) (0.17.1)\n",
            "Collecting python-Levenshtein==0.12.2 (from pyiextract)\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langdetect==1.0.9 in /usr/local/lib/python3.10/dist-packages (from pyiextract) (1.0.9)\n",
            "Collecting torch<1.12.0,>=1.10.0 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.13.0,>=0.8.1 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting fairscale==0.4.6 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (3.8.1)\n",
            "Requirement already satisfied: spacy<3.4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (3.3.3)\n",
            "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (1.23.5)\n",
            "Collecting tensorboardX>=1.2 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (4.66.1)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (1.11.4)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (7.4.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (0.1.99)\n",
            "Collecting filelock<3.8,>=3.3 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting lmdb>=1.2.1 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (10.1.0)\n",
            "Collecting termcolor==1.1.0 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb<0.13.0,>=0.10.0 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (0.20.2)\n",
            "Collecting dill>=0.3.4 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting base58>=2.1.1 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (0.1.1)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (0.4.2)\n",
            "Collecting rich==12.1 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading rich-12.1.0-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.8/229.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.20.0 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp==2.10.0->pyiextract) (5.7.1)\n",
            "Collecting jsonnet>=0.10.0 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting conllu==4.4.2 (from allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting word2number>=1.1 (from allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py-rouge==1.1 (from allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from allennlp-models==2.10.0->pyiextract) (6.1.3)\n",
            "Collecting datasets (from allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect==1.0.9->pyiextract) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein==0.12.2->pyiextract) (69.0.3)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy==0.12.0->pyiextract) (5.3.2)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy==0.12.0->pyiextract) (2.0.10)\n",
            "Collecting cytoolz>=0.10.1 (from textacy==0.12.0->pyiextract)\n",
            "  Downloading cytoolz-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting jellyfish>=0.8.0 (from textacy==0.12.0->pyiextract)\n",
            "  Downloading jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy==0.12.0->pyiextract) (1.3.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy==0.12.0->pyiextract) (3.2.1)\n",
            "Collecting pyphen>=0.10.0 (from textacy==0.12.0->pyiextract)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1->pyiextract) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1->pyiextract) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1->pyiextract) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1->pyiextract) (0.12.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich==12.1->allennlp==2.10.0->pyiextract)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich==12.1->allennlp==2.10.0->pyiextract) (2.16.1)\n",
            "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (1.34.18)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (2.8.0)\n",
            "Collecting huggingface-hub>=0.0.16 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy==0.12.0->pyiextract) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp==2.10.0->pyiextract) (4.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp==2.10.0->pyiextract) (8.1.7)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.0->pyiextract) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.0->pyiextract) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.0->pyiextract) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp==2.10.0->pyiextract) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.0->pyiextract) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.0->pyiextract) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.0->pyiextract) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp==2.10.0->pyiextract) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp==2.10.0->pyiextract) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (0.7.11)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (2.4.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.13.0,>=0.8.1->allennlp==2.10.0->pyiextract) (9.4.0)\n",
            "Collecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->allennlp-models==2.10.0->pyiextract) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->allennlp-models==2.10.0->pyiextract) (0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->allennlp-models==2.10.0->pyiextract) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->allennlp-models==2.10.0->pyiextract) (3.4.1)\n",
            "Collecting multiprocess (from datasets->allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->allennlp-models==2.10.0->pyiextract) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->allennlp-models==2.10.0->pyiextract) (3.9.1)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets (from allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.3-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading datasets-2.14.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.13.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting dill>=0.3.4 (from allennlp==2.10.0->pyiextract)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl.metadata (20 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading datasets-2.13.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets->allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->allennlp-models==2.10.0->pyiextract) (0.2.12)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.18 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (1.34.18)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (0.10.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->allennlp-models==2.10.0->pyiextract) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->allennlp-models==2.10.0->pyiextract) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->allennlp-models==2.10.0->pyiextract) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->allennlp-models==2.10.0->pyiextract) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->allennlp-models==2.10.0->pyiextract) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->allennlp-models==2.10.0->pyiextract) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (2.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp==2.10.0->pyiextract) (2.1.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets->allennlp-models==2.10.0->pyiextract)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->allennlp-models==2.10.0->pyiextract) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->allennlp-models==2.10.0->pyiextract) (2023.3.post1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp==2.10.0->pyiextract)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (1.62.0)\n",
            "INFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract)\n",
            "  Downloading google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading google_api_core-2.13.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading google_api_core-2.13.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_api_core-2.10.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (1.5.0)\n",
            "INFO: pip is looking at multiple versions of googleapis-common-protos to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting googleapis-common-protos<2.0dev,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.0/223.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.57.1-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.0/218.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.0/218.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of googleapis-common-protos to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.0->pyiextract) (0.5.1)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'rich' candidate (version 12.1.0 at https://files.pythonhosted.org/packages/bc/be/1ace556afa0cf17599c2a631b04b280ae7502a9cf942c47fd66ca9ab5134/rich-12.1.0-py3-none-any.whl (from https://pypi.org/simple/rich/) (requires-python:>=3.6.2,<4.0.0))\n",
            "Reason for being yanked: Broken dependencies. Please upgrade to 12.2.0 or later\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading cytoolz-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pyiextract, python-Levenshtein, fairscale, termcolor, jsonnet, word2number, pathtools\n",
            "  Building wheel for pyiextract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyiextract: filename=pyiextract-0.0.4-py3-none-any.whl size=34669 sha256=bf5fe4bba69c2d427c35ae623192bc5fe800e65179a45129ea277ecbb43fe2a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/a1/0b/dc1afc1d02cc1ab017286a9d0f4f2682e44901f86183186e2b\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp310-cp310-linux_x86_64.whl size=159970 sha256=cecaeb1dece006c6142092077b9a0e6e12fc5a27c0e6b117cf0761908abbe161\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/c3/05/60b4747cf52e0f6b6ee52022088a4de07d755016493e86373d\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307222 sha256=1d5fc4365ee386408e32ac733110b207db12da8a7ad1fcff2915346d1afc0c23\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=4f5ee89bbd0099dcb4ec55e07bd4f9805d6ac8c157d387096b5cdd4d8e8317ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406861 sha256=225c71c15ba6b0989b4c56c8a7871666e60a8c3c0f021635764e82f2a2f9a79b\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=d25877490448eab2f5c0d625392c4b04394138d7cf9ff80d0673af1f508c2487\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=b9e49f79dc2844eba9f5ddb8dabdd977433eea11dc99b003924dfd44491ffd04\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pyiextract python-Levenshtein fairscale termcolor jsonnet word2number pathtools\n",
            "Installing collected packages: word2number, termcolor, py-rouge, pathtools, lmdb, jsonnet, commonmark, torch, smmap, shortuuid, setproctitle, sentry-sdk, rich, python-Levenshtein, pyphen, protobuf, jellyfish, filelock, docker-pycreds, dill, cytoolz, conllu, base58, torchvision, tensorboardX, responses, multiprocess, huggingface-hub, googleapis-common-protos, gitdb, fairscale, google-api-core, GitPython, wandb, textacy, datasets, cached-path, allennlp, allennlp-models, pyiextract\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.4.0\n",
            "    Uninstalling termcolor-2.4.0:\n",
            "      Successfully uninstalled termcolor-2.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.0\n",
            "    Uninstalling rich-13.7.0:\n",
            "      Successfully uninstalled rich-13.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.1\n",
            "    Uninstalling protobuf-3.20.1:\n",
            "      Successfully uninstalled protobuf-3.20.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: conllu\n",
            "    Found existing installation: conllu 4.5.3\n",
            "    Uninstalling conllu-4.5.3:\n",
            "      Successfully uninstalled conllu-4.5.3\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.2\n",
            "    Uninstalling huggingface-hub-0.20.2:\n",
            "      Successfully uninstalled huggingface-hub-0.20.2\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.62.0\n",
            "    Uninstalling googleapis-common-protos-1.62.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.62.0\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.1\n",
            "    Uninstalling google-api-core-2.11.1:\n",
            "      Successfully uninstalled google-api-core-2.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.4.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.39.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-iam 2.13.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-iam 2.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.11.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-resource-manager 1.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.8.2 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "ibis-framework 7.1.0 requires rich<14,>=12.4.4, but you have rich 12.1.0 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 2.8.2 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.41 allennlp-2.10.0 allennlp-models-2.10.0 base58-2.1.1 cached-path-1.1.6 commonmark-0.9.1 conllu-4.4.2 cytoolz-0.12.2 datasets-2.10.1 dill-0.3.6 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.11 google-api-core-2.8.2 googleapis-common-protos-1.56.4 huggingface-hub-0.10.1 jellyfish-1.0.3 jsonnet-0.20.0 lmdb-1.4.1 multiprocess-0.70.14 pathtools-0.1.2 protobuf-3.20.0 py-rouge-1.1 pyiextract-0.0.4 pyphen-0.14.0 python-Levenshtein-0.12.2 responses-0.18.0 rich-12.1.0 sentry-sdk-1.39.2 setproctitle-1.3.3 shortuuid-1.0.11 smmap-5.0.1 tensorboardX-2.6.2.2 termcolor-1.1.0 textacy-0.12.0 torch-1.11.0 torchvision-0.12.0 wandb-0.12.21 word2number-1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install urllib3 --upgrade\n",
        "!pip install opennre"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC_mDUXDoxI6",
        "outputId": "4d044c88-ab70-47d9-dbe7-3ad13544efde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opennre (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opennre\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyiextract import pipeline\n",
        "\n",
        "# from pyiextract.blinknormaliser import BlinkNormaliser\n",
        "from pyiextract.coreferencenormaliser import CoreferenceNormaliser\n",
        "from pyiextract.coreferencereducer import CoreferenceReducer\n",
        "from pyiextract.englishnormaliser import EnglishNormaliser\n",
        "from pyiextract.llmextractor import LLMExtractor\n",
        "from pyiextract.nerreducer import NERReducer\n",
        "from pyiextract.oiextractor import OIExtractor\n",
        "from pyiextract.pipeline import Pipeline\n",
        "from pyiextract.subjectivityreducer import SubjectivityReducer\n",
        "from pyiextract.svoextractor import SVOExtractor\n",
        "\n",
        "pipeline = Fullpipeline.fullpipeline([EnglishNormaliser(), CoreferenceNormaliser()], [SVOExtractor(), OIExtractor()],[CoreferenceReducer(), NERReducer(), SubjectivityReducer()])\n",
        "doc = pipeline.extract(\"Avram Noam Chomsky ( born December 7 , 1928 ) is an American professor and public intellectual known for his work in linguistics , political activism , and social criticism .\")\n",
        "print(json.dumps([str(x) for x in doc.triples()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596,
          "referenced_widgets": [
            "6613052cd2b74b089e9447f1bd1d82f1",
            "0f7638a08eec4158a03314fd182e7731"
          ]
        },
        "id": "adPHVbqgkT29",
        "outputId": "de09a316-e296-462e-f260-12e852f182c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6613052cd2b74b089e9447f1bd1d82f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ConfigValidationError",
          "evalue": "\n\nConfig validation error\n\ntagger -> label_smoothing   extra fields not permitted\n\n{'nlp': <spacy.lang.en.English object at 0x7a7f55e57850>, 'name': 'tagger', 'label_smoothing': 0.0, 'model': {'@architectures': 'spacy.Tagger.v2', 'nO': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy.Tok2VecListener.v1', 'width': 96, 'upstream': 'tok2vec'}}, 'neg_prefix': '!', 'overwrite': False, 'scorer': {'@scorers': 'spacy.tagger_scorer.v1'}, '@factories': 'tagger'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConfigValidationError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-4ef24879bd99>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyiextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvoextractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVOExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEnglishNormaliser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSVOExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOIExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCoreferenceReducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNERReducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubjectivityReducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Avram Noam Chomsky ( born December 7 , 1928 ) is an American professor and public intellectual known for his work in linguistics , political activism , and social criticism .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyiextract/oiextractor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openie\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         self._predictor = Predictor.from_path(\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;34m\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mfrom_path\u001b[0;34m(cls, archive_path, predictor_name, cuda_device, dataset_reader_to_load, frozen, import_plugins, overrides, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimport_plugins\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         return Predictor.from_archive(\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0mload_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mpredictor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mfrom_archive\u001b[0;34m(cls, archive, predictor_name, dataset_reader_to_load, frozen, extra_args)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/allennlp_models/structured_prediction/predictors/srl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, dataset_reader, language)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_language\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpacyTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/allennlp/data/tokenizers/spacy_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, pos_tags, parse, ner, keep_spacy_tokens, split_on_spaces, start_tokens, end_tokens)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_on_spaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_spaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_spacy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_on_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/allennlp/common/util.py\u001b[0m in \u001b[0;36mget_spacy_model\u001b[0;34m(spacy_model_name, pos_tags, parse, ner)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mdisable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mspacy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             logger.warning(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blank:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \"\"\"\n\u001b[1;32m    452\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     nlp = load_model_from_config(\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_config\u001b[0;34m(config, meta, vocab, disable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;31m# registry, including custom subclasses provided via entry points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0mlang_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     nlp = lang_cls.from_config(\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, vocab, disable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                     \u001b[0;31m# The pipe name (key in the config) here is the unique name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                     \u001b[0;31m# of the component, not necessarily the factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     nlp.add_pipe(\n\u001b[0m\u001b[1;32m   1810\u001b[0m                         \u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipe_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    793\u001b[0m                     \u001b[0mlang_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 )\n\u001b[0;32m--> 795\u001b[0;31m             pipe_component = self.create_pipe(\n\u001b[0m\u001b[1;32m    796\u001b[0m                 \u001b[0mfactory_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;31m# We're calling the internal _fill here to avoid constructing the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;31m# registered functions twice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mresolved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     ) -> Dict[str, Any]:\n\u001b[0;32m--> 746\u001b[0;31m         resolved, _ = cls._make(\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m_make\u001b[0;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_interpolated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         filled, _, resolved = cls._fill(\n\u001b[0m\u001b[1;32m    796\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m_fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[1;32m    848\u001b[0m                     \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_model_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0mpromise_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_promise_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m                 filled[key], validation[v_key], final[key] = cls._fill(\n\u001b[0m\u001b[1;32m    851\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0mpromise_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m_fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m                 raise ConfigValidationError(\n\u001b[0m\u001b[1;32m    917\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m                 ) from None\n",
            "\u001b[0;31mConfigValidationError\u001b[0m: \n\nConfig validation error\n\ntagger -> label_smoothing   extra fields not permitted\n\n{'nlp': <spacy.lang.en.English object at 0x7a7f55e57850>, 'name': 'tagger', 'label_smoothing': 0.0, 'model': {'@architectures': 'spacy.Tagger.v2', 'nO': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy.Tok2VecListener.v1', 'width': 96, 'upstream': 'tok2vec'}}, 'neg_prefix': '!', 'overwrite': False, 'scorer': {'@scorers': 'spacy.tagger_scorer.v1'}, '@factories': 'tagger'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1QARlneZmqNsJhwTA3mFa9MGMh8CNm8QJ"
      ],
      "metadata": {
        "id": "DkdcaMTM7azu",
        "outputId": "c2ca5168-1248-44ba-f996-876ab082ad89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gdown/cli.py\", line 151, in main\n",
            "    filename = download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gdown/download.py\", line 203, in download\n",
            "    filename_from_url = m.groups()[0]\n",
            "AttributeError: 'NoneType' object has no attribute 'groups'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "def get_matches(tuple1, tuple2):\n",
        "    \"\"\"\n",
        "    returns nb of matches between tuple1 and tuple2 (token level)\n",
        "    returns 0 if there is an argument without token match.\n",
        "    \"\"\"\n",
        "    if len(tuple1) != len(tuple2):\n",
        "        return 0\n",
        "    total_matches = 0\n",
        "\n",
        "    for arg_tuple1, arg_tuple2 in zip(tuple1, tuple2):\n",
        "        arg_tuple1 = re.sub('\\\\.*\\[.+\\]', '', arg_tuple1)\n",
        "        arg_tuple2 = re.sub('\\\\.*\\[.+\\]', '', arg_tuple2)\n",
        "\n",
        "        arg_tuple1 = arg_tuple1.split(\" \")\n",
        "        arg_tuple2 = arg_tuple2.split(\" \")\n",
        "        matches = [value for value in arg_tuple1 if value in arg_tuple2]\n",
        "\n",
        "        if len(matches) == 0:\n",
        "            return 0\n",
        "\n",
        "        total_matches += len(matches)\n",
        "\n",
        "    return total_matches\n",
        "\n",
        "\n",
        "def F1_scorer(annotations_pred, annotations_gold , weighted=False):\n",
        "    \"\"\"\n",
        "    annotations_pred: list[list[list[str]]] all annotated sentences from annotator, each sentence is a list of list of strings (=a list of tuples)\n",
        "    annotations_gold: list[list[list[str]]] all annotated sentences from gold corpus, each sentence is a list of list of strings (=a list of tuples)\n",
        "    \"\"\"\n",
        "\n",
        "    precision_sys_numerator = 0\n",
        "    precision_sys_denominator = 0\n",
        "    recall_sys_numerator = 0\n",
        "    recall_sys_denominator = 0\n",
        "\n",
        "    #work on each sentence (list of tuples)\n",
        "    for sentence_preds, sentence_golds in zip(annotations_pred, annotations_gold):\n",
        "\n",
        "        precision_sentence_numerator = 0\n",
        "        precision_sentence_denominator = 0\n",
        "        recall_sentence_numerator = 0\n",
        "        recall_sentence_denominator = 0\n",
        "\n",
        "        while len(sentence_preds) > 0 and len(sentence_golds) > 0:\n",
        "\n",
        "            #get for each predicted tuple its lenght\n",
        "            pred_lengths = [len(\" \".join(tup).split(\" \")) for tup in sentence_preds]\n",
        "            gold_lengths = [len(\" \".join(tup).split(\" \")) for tup in sentence_golds]\n",
        "            f1_sentence = []\n",
        "\n",
        "            #get for EACH predicted tuple its F1 score with EACH gold tuple:\n",
        "            for index_pred, pred_tuple in enumerate(sentence_preds):\n",
        "                f1_tuple = []\n",
        "                for index_gold, gold_tuple in enumerate(sentence_golds):\n",
        "                    ##  get number of matching tokens between all args of predicted tuple and eall args of all gold tuples\n",
        "                    matches = get_matches(pred_tuple, gold_tuple)\n",
        "\n",
        "                    ##  get recall and precision with each tuple\n",
        "                    recall = matches / gold_lengths[index_gold]\n",
        "                    precision = matches / pred_lengths[index_pred]\n",
        "\n",
        "                    ## get F1 scores\n",
        "                    if (precision+recall == 0):\n",
        "                        f1_tuple.append(0)\n",
        "                    else:\n",
        "                        f1_tuple.append((2 * precision * recall) / (precision + recall))\n",
        "\n",
        "                f1_sentence.append(f1_tuple)\n",
        "\n",
        "            # and get argmax (delete predicted tuple and corresponding gold tuple)\n",
        "            pred_index = numpy.argmax([max(tuples_f1) for tuples_f1 in f1_sentence])\n",
        "            gold_index = numpy.argmax(f1_sentence[pred_index])\n",
        "\n",
        "            precision_sentence_numerator += get_matches(sentence_preds[pred_index], sentence_golds[gold_index])\n",
        "            recall_sentence_numerator += get_matches(sentence_preds[pred_index], sentence_golds[gold_index])\n",
        "            precision_sentence_denominator += pred_lengths[pred_index]\n",
        "            recall_sentence_denominator += gold_lengths[gold_index]\n",
        "\n",
        "\n",
        "            sentence_preds.pop(pred_index)\n",
        "            sentence_golds.pop(gold_index)\n",
        "\n",
        "            pred_lengths.pop(pred_index)\n",
        "            gold_lengths.pop(gold_index)\n",
        "\n",
        "        #increase sys numerators and denominators\n",
        "        precision_sys_numerator += precision_sentence_numerator\n",
        "        precision_sys_denominator += precision_sentence_denominator\n",
        "        recall_sys_numerator += recall_sentence_numerator\n",
        "        recall_sys_denominator += recall_sentence_denominator\n",
        "\n",
        "    precision = precision_sys_numerator / precision_sys_denominator\n",
        "    recall = recall_sys_numerator / recall_sys_denominator\n",
        "\n",
        "    #return overall F1\n",
        "    return (2 * precision * recall ) / (precision + recall)\n",
        "\n",
        "\n",
        "preds = [[[\"a a a\",\"b b b\", \"c c c\"], [\"a a a\", \"b b b\", \"c d d\"], [\"b b b\", \"c c c\", \"g f r\", \"t t t\"]]]\n",
        "golds = [[[\"a a a\", \"a b a\", \"d g g\"], [\"a a a\",\"b b b\", \"c c c\"]]]\n",
        "\n",
        "F1_scorer(preds, golds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6XyfQr58I8O",
        "outputId": "1a2e4ede-d38f-477a-9525-971ee2da31b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9444444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "f1 = (2*0.5*0.5) / (0.5 + 0.5)\n",
        "f2 = (2*0.4*0.3) / (0.4 + 0.3)\n",
        "\n",
        "p = 0.4 + 0.5\n",
        "r = 0.5 + 0.3\n",
        "\n",
        "F = ((2*p*r) / (p+r) )/2\n",
        "print(\"F:\", F)\n",
        "print(\"summed:\", (f1+f2)/2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-cOUZf_KPXA",
        "outputId": "34c4a625-f01d-4ba2-9a62-1c128e7d6216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F: 0.4235294117647059\n",
            "summed: 0.42142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "arg_tuple1 = \"Chomsky / ( A A A) \"\n",
        "arg_tuple1 = re.sub('/.*\\(.+\\)', '', arg_tuple1)\n",
        "print(arg_tuple1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l6uJq69Rjdt",
        "outputId": "05fa66da-a9db-4541-fedf-c97a2895a7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chomsky \n"
          ]
        }
      ]
    }
  ]
}