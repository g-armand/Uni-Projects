{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n"
      ],
      "metadata": {
        "id": "QYKZ6VvLiHj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "swaqGQjdwRto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d615848c-eef3-43bf-a2cc-5e4d50c258dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/118.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m112.6/118.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.3.1\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=159271f8d34e526146da569b1a9ec2e15a6a8a250097c67de719b596274f0a57\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sacrebleu.tokenizers import tokenizer_char, tokenizer_spm, tokenizer_none\n",
        "import sacrebleu\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from transformers import MarianTokenizer, MarianMTModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCt2-NpHiHOO",
        "outputId": "9d9c4459-95f7-421f-a57a-8ff60f43aacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define BLEU metric\n"
      ],
      "metadata": {
        "id": "vjnvf3NmiORU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_bleu(reference_list, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "    # Calculate n-gram precision for n=1, 2, 3, and 4\n",
        "    precisions = []\n",
        "    if type(candidate) == str:\n",
        "        return candidate in reference_list\n",
        "\n",
        "    for n in range(1, len(weights)+1):\n",
        "        #Count the number of candidate n-grams that appear in the references\n",
        "        candidate_ngrams = [tuple(candidate[i:i + n]) for i in range(len(candidate) - n + 1)]\n",
        "        candidate_ngram_counts = collections.Counter(candidate_ngrams)\n",
        "\n",
        "        #Calculate the maximum n-gram count in the reference sentences\n",
        "        reference_ngram_counts = collections.Counter()\n",
        "\n",
        "        for reference in reference_list:\n",
        "            reference_ngrams = [tuple(reference[i:i + n]) for i in range(len(reference) - n + 1)]\n",
        "            ngram_counts = collections.Counter(reference_ngrams)\n",
        "\n",
        "            for key, value in ngram_counts.items():\n",
        "                reference_ngram_counts[key] = max(reference_ngram_counts.get(key, 0), value)\n",
        "\n",
        "        for candidate_ngram in candidate_ngram_counts.keys():\n",
        "            if candidate_ngram in reference_ngram_counts.keys():\n",
        "                if candidate_ngram_counts[candidate_ngram] > reference_ngram_counts[candidate_ngram]:\n",
        "                    candidate_ngram_counts[candidate_ngram] = reference_ngram_counts[candidate_ngram]\n",
        "            else:\n",
        "                candidate_ngram_counts[candidate_ngram] = 0\n",
        "\n",
        "        denominator = 0.0000000001 if len(candidate_ngrams) ==0 else len(candidate_ngrams)\n",
        "        precision = sum(candidate_ngram_counts.values()) / denominator\n",
        "        precisions.append(precision)\n",
        "\n",
        "    #Calculate brevity penalty\n",
        "    reference_lengths = [len(reference) for reference in reference_list]\n",
        "    candidate_length = len(candidate)\n",
        "    closest_reference_length = min(reference_lengths, key=lambda x: abs(x - candidate_length))\n",
        "    brevity_penalty = min(1, math.exp(1 - closest_reference_length / candidate_length))\n",
        "\n",
        "    geometric_mean = math.exp(sum(weights[index] * math.log(precisions[index]) for index in range(len(precisions)) if precisions[index]>0))\n",
        "    bleu = brevity_penalty * geometric_mean\n",
        "\n",
        "    return bleu"
      ],
      "metadata": {
        "id": "ixvVpndMiQpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retrieve translations\n"
      ],
      "metadata": {
        "id": "DWKduEXZiUjE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__j5VoTYnrDi"
      },
      "outputs": [],
      "source": [
        "with open(\"lab3/translations_bilingual.txt\", 'r', encoding=\"utf-8\") as file:\n",
        "    bilingual_translations = file.readlines()\n",
        "with open(\"lab3/translations_multilingual.txt\", 'r', encoding=\"utf-8\") as file:\n",
        "    multilingual_translations = file.readlines()\n",
        "with open(\"lab3/newsdiscusstest2015-fren-src.fr.sgm\", 'r', encoding=\"utf-8\") as file:\n",
        "    french_sentences = file.readlines()\n",
        "with open(\"lab3/newsdiscusstest2015-fren-ref.en.sgm\", 'r', encoding=\"utf-8\") as file:\n",
        "    english_sentences = file.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenize translations and gold references"
      ],
      "metadata": {
        "id": "AP0otqNQio-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bilingual_translations = [nltk.word_tokenize(sentence.strip().lower()) for sentence in bilingual_translations]\n",
        "multilingual_translations = [nltk.word_tokenize(sentence.strip().lower()) for sentence in multilingual_translations]\n",
        "french_sentences = [nltk.word_tokenize(sentence.strip().lower()) for sentence in french_sentences]\n",
        "english_sentences = [nltk.word_tokenize(sentence.strip().lower()) for sentence in english_sentences]"
      ],
      "metadata": {
        "id": "1eoUF99_inox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate BLEU scores"
      ],
      "metadata": {
        "id": "8RI-I1-7ix3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bilingual_bleu = [calculate_bleu([gold], pred) for gold, pred in zip(english_sentences, bilingual_translations)]\n",
        "multilingual_bleu = [calculate_bleu([gold], pred) for gold, pred in zip(english_sentences, multilingual_translations)]\n",
        "bilingual_bleu = sum(bilingual_bleu) / len(bilingual_bleu)\n",
        "multilingual_bleu = sum(multilingual_bleu) / len(multilingual_bleu)\n",
        "print(f\"bilingual_bleu: {str(bilingual_bleu)}, multilingual_bleu: {str(multilingual_bleu)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6nZpBPcixlA",
        "outputId": "e4c92c1c-480e-417d-ef63-0b1fb99b4d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bilingual_bleu: 0.48434392672277704, multilingual_bleu: 0.46088272183263895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Find all permutations for all sentences in the corpus"
      ],
      "metadata": {
        "id": "vRLtNxSci7xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_permutations(candidate, reference):\n",
        "\n",
        "    #Calculate the maximum n-gram count in the reference sentences\n",
        "    reference_ngram_counts = collections.Counter()\n",
        "    reference_ngrams = [tuple(reference[i:i + 2]) for i in range(len(reference) - 2 + 1)]\n",
        "    ngram_counts = collections.Counter(reference_ngrams)\n",
        "    for key, value in ngram_counts.items():\n",
        "        reference_ngram_counts[key] = max(reference_ngram_counts.get(key, 0), value)\n",
        "\n",
        "    #Count the number of candidate n-grams that appear in the references\n",
        "    candidate_ngrams = [tuple(candidate[i:i + 2]) for i in range(len(candidate) - 2 + 1)]\n",
        "    candidate_ngram_counts = collections.Counter(candidate_ngrams)\n",
        "    for candidate_ngram in candidate_ngram_counts.keys():\n",
        "        if candidate_ngram in reference_ngram_counts.keys():\n",
        "            if candidate_ngram_counts[candidate_ngram] > reference_ngram_counts[candidate_ngram]:\n",
        "                candidate_ngram_counts[candidate_ngram] = reference_ngram_counts[candidate_ngram]\n",
        "        else:\n",
        "            candidate_ngram_counts[candidate_ngram] = 0\n",
        "    unmatching_bigrams = sum(elt+1 for elt in candidate_ngram_counts.values() if elt==0)\n",
        "    return math.factorial(unmatching_bigrams + 1)\n",
        "\n",
        "#basic exemple\n",
        "reference = [\"le\", \"chat\", \"mange\", \"le\", \"mulot\",\"rouge\"]\n",
        "candidate = [\"le\", \"chat\", \"voit\", \"le\", \"mulot\", \"marron\"]\n",
        "\n",
        "print(f\"There are {get_all_permutations(reference, candidate)} permutations possibles of \\\"{' '.join(candidate)}\\\"\")\n",
        "\n",
        "#all corpus\n",
        "permutations_count = 0\n",
        "for pred, gold in zip(bilingual_translations, english_sentences):\n",
        "    permutations_count += get_all_permutations(pred, gold)\n",
        "print(\"All corpus mean permutations per candidate\", permutations_count/len(bilingual_translations))\n",
        "print(\"All corpus total permutations \",permutations_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTAGLEUNi6FN",
        "outputId": "44e2785c-c54d-4801-ff26-8356828ee594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 24 permutations possibles of \"le chat voit le mulot marron\"\n",
            "All corpus mean permutations per candidate 9.01334351785624e+136\n",
            "All corpus total permutations  135200152767843610495301134112762141535165996608194809310176793339697717688179906744929245388468309137864148102967187311809322666711665144418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute SacreBLEU on our corpora\n"
      ],
      "metadata": {
        "id": "9q3l0zmljvvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lab3/translations_bilingual.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  bilingual_translations = file.readlines()\n",
        "with open(\"lab3/translations_multilingual.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  multilingual_translations = file.readlines()\n",
        "with open(\"lab3/newsdiscusstest2015-fren-ref.en.sgm\", \"r\", encoding=\"utf-8\") as file:\n",
        "  english_sentences = file.readlines()\n",
        "\n",
        "bilingual_sacrebleu_score = sacrebleu.corpus_bleu(bilingual_translations, [[elt] for elt in english_sentences], tokenize=\"13a\").score\n",
        "multilingual_sacrebleu_score = sacrebleu.corpus_bleu(multilingual_translations, [[elt] for elt in english_sentences], tokenize=\"13a\").score\n",
        "print(f\"bilingual SacreBLEU:{bilingual_sacrebleu_score}, multilingual SacreBLEU: {multilingual_sacrebleu_score}\")\n",
        "\n",
        "i=0\n",
        "# subword units\n",
        "tokenizer = tokenizer_spm.Flores101Tokenizer()\n",
        "bilingual_bleu_score = 0\n",
        "for pred_bi, gold in zip(bilingual_translations, english_sentences):\n",
        "    bilingual_bleu_score += calculate_bleu([tokenizer(gold).split()], tokenizer(pred_bi).split())\n",
        "bilingual_sacrebleu_score = sacrebleu.corpus_bleu(bilingual_translations, [[elt] for elt in english_sentences], tokenize=\"flores101\").score\n",
        "print(f\"SUBWORD UNITS -> BLEU: {bilingual_bleu_score/len(bilingual_translations)}, sacreBLEU {bilingual_sacrebleu_score}\")\n",
        "\n",
        "\n",
        "# none\n",
        "tokenizer = sacrebleu.tokenizers.tokenizer_none.NoneTokenizer()\n",
        "bilingual_bleu_score = 0\n",
        "for pred_bi, gold in zip(bilingual_translations, english_sentences):\n",
        "    bilingual_bleu_score += calculate_bleu([gold], pred_bi)\n",
        "\n",
        "bilingual_sacrebleu_score = sacrebleu.raw_corpus_bleu(bilingual_translations, [[elt] for elt in english_sentences]).score\n",
        "print(f\"NONE -> BLEU: {bilingual_bleu_score/len(bilingual_translations)}, SacreBLEU: {bilingual_sacrebleu_score}\")\n",
        "\n",
        "\n",
        "# char\n",
        "tokenizer = tokenizer_char.TokenizerChar()\n",
        "bilingual_bleu_score = 0\n",
        "bilingual_sacrebleu_score = 0\n",
        "for pred_bi, gold in zip(bilingual_translations, english_sentences):\n",
        "    bilingual_bleu_score += calculate_bleu([[*gold]], [*pred_bi])\n",
        "bilingual_sacrebleu_score = sacrebleu.corpus_bleu(bilingual_translations, [[elt] for elt in english_sentences], tokenize=\"char\").score\n",
        "\n",
        "print(f\"CHAR -> BLEU: {bilingual_bleu_score/len(bilingual_translations)}, SacreBLEU: {bilingual_sacrebleu_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "m4aA7X0GjvYB",
        "outputId": "e8b23d70-3f93-4793-d651-723ccde27c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:Tokenizer 'spm' has been changed to 'flores101', and may be removed in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bilingual SacreBLEU:13.006392202018965, multilingual SacreBLEU: 13.380161378318961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:Tokenizer 'spm' has been changed to 'flores101', and may be removed in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUBWORD UNITS -> BLEU: 0.472146790197805, sacreBLEU 12.70331870386537\n",
            "NONE -> BLEU: 0.051333333333333335, SacreBLEU: 6.948413844794133\n",
            "CHAR -> BLEU: 0.6689664270316409, SacreBLEU: 94.49905230826586\n"
          ]
        }
      ]
    }
  ]
}