{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspjPrcoERKm"
      },
      "source": [
        "# Projet TextRank\n",
        "**Avant toute chose, assurez vous de copier ce notebook et de travailler uniquement sur votre copie personnelle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fhAYZWLN3V7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or5sIbeiVbvu"
      },
      "source": [
        "Préambule\n",
        "---------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87vpAdB8HMtm"
      },
      "source": [
        "Ce préambule télécharge le dictionnaire word2vec qui associe des mots (`str`) à des vecteurs (`np.array `de taille 200)\n",
        "Il télécharge aussi le corpus de travail pour ce TP qui est issu de wikipedia\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#INIT notebook\n",
        "!pip install gensim\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "#Download word vectors if needed (this may take time)\n",
        "if 'frw2v.bin' not in os.listdir():\n",
        "  urlretrieve('https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin','frw2v.bin')\n",
        "w2v = KeyedVectors.load_word2vec_format(\"frw2v.bin\",binary=True)\n",
        "\n",
        "#this list stores the 65 most frequent french words, will be used later to avoid too frequent words in sentence vector processing\n",
        "very_frequent_words = [list(w2v.vocab.keys())[i] for i in range(65)]"
      ],
      "metadata": {
        "id": "5EkxKb0_NsQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f22075e-70e3-4e44-a222-5e57c62bbaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFi1Dg5SMPPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441d3e75-ec47-477c-b7d1-b8a05f8bca3e"
      },
      "source": [
        "#Téléchargement du corpus wikipedia\n",
        "!pip install wikipedia\n",
        "import wikipedia\n",
        "\n",
        "wikipedia.set_lang('fr')\n",
        "\n",
        "writers = ['Corneille',\"Racine\",'Flaubert','Balzac','Zola','Baudelaire','Rimbaud','Verlaine']\n",
        "corpus  = [ wikipedia.summary(auteur) for auteur in writers ]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=e7749a4ccc836b26bf0d65907ad3c5396bdbdb1c711edd9f954ee56c49b7090f\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Twbv5MdkZ0"
      },
      "source": [
        "A ce stade, la liste `corpus` contient les textes, le dictionnaire `w2v` associe des mots à des vecteurs de taille 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCNYlM-cdxTD"
      },
      "source": [
        "Premier exercice: segmentation en phrases et en mots\n",
        "---------------\n",
        "\n",
        "Définir une fonction qui découpe un texte en phrases et qui renvoie une liste de phrases.\n",
        "Définir une autre fonction qui découpe une phrase en mots. Celle-ci renvoie une liste de mots\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U0jl91feHE3"
      },
      "source": [
        "#basic functions, might need to make them more complex but work fine for now\n",
        "def text_tokenize(texte):\n",
        "  return texte.split(\".\")\n",
        "\n",
        "def sent_tokenize(sentence):\n",
        "  #sentence cleaning\n",
        "  sentence = sentence.lower()\n",
        "  sentence = sentence.replace(',', '')\n",
        "  sentence = sentence.replace('.', '')\n",
        "  sentence = sentence.replace('(', '')\n",
        "  sentence = sentence.replace(')', '')\n",
        "  sentence = sentence.replace(\"’\",\"'\")\n",
        "\n",
        "  #make clean tokens\n",
        "  sentence = sentence.split(\" \")\n",
        "  for index in range(len(sentence)):\n",
        "    sentence[index] = sentence[index].strip()\n",
        "    if len(sentence[index]) >2 and sentence[index][1] == \"'\": #delete l', d', j', s', ...\n",
        "      sentence[index] = sentence[index][2:]\n",
        "\n",
        "  return sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekmSH86Kep0P"
      },
      "source": [
        "Second exercice: calculer une représentation vectorielle de la phrase par sac de mots\n",
        "--------------------\n",
        "\n",
        "Définir une fonction qui prend en argument une liste de mots et qui renvoie un vecteur. Faire attention à gérer le problèmes des mots inconnus du dictionnaire. Faut-il inclure tous les mots pour obtenir un vecteur de phrase informatif ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on fais le choix de créer un vecteur principal de taille 200(la taille des vecteurs donnés par w2v.).\n",
        "Pour chaque mot rencontré, si un vecteur lui est associé dans w2v, on additione ce vecteur au vecteur principal. Enfin on divise chaque valeur du vecteur par 1/(nombre de mots), ce qui nous donne un vecteur 'moyen'."
      ],
      "metadata": {
        "id": "KcpXy84GTq71"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nToasuPzfM9k"
      },
      "source": [
        "def vectorize_sent(token_list, vocab):\n",
        "\n",
        "  #sentence vector\n",
        "  vector = np.zeros(200)\n",
        "  count = 0\n",
        "  for token in token_list:\n",
        "    if token in vocab and token not in very_frequent_words:\n",
        "      #sentence vector update\n",
        "      vector = vector + vocab[token]\n",
        "      count += 1\n",
        "\n",
        "  if count == 0:\n",
        "    return np.zeros(200)\n",
        "  else:\n",
        "    #each dimension of sentence vector has to be divided by number of vectors to obtain its \"average\" value\n",
        "    return vector.dot(1/count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWPjow1hfYWs"
      },
      "source": [
        "Troisième exercice : Définir une fonction qui calcule les similarités entre les phrases\n",
        "--------------\n",
        "Définir une fonction qui, à partir d'une liste de phrases, calcule une matrice stochastique construite à partir des similarités cosinus entre chaque couple de phrases. Faut-il réaliser un lissage comme dans le cas du pagerank traditionnel ? pourquoi ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici il n'y a pas besoin d'appliquer un lissage, car aucun des vecteurs de chaque mot de la phrase ne contient 0. Ainsi, après calcul des similarités avec le cosinus, il n'y a pas de valeurs nulle dans la matrice.\n",
        "Dans le cas d'une approche par sac de mot traditionelle où on compterais les occurences de chaque mot, il est probable que certains vecteurs aient une similarité de 0, car aucun des mots ne correspondrait. Voilà pourquoi on laisse la possibilité de changer la valeur de delta, afin d'appliquer un lissage comme dans le cas du pagerank traditionnel, si jamais on préfèrais d'aborder le problème avec des vecteurs de fréquence.\n"
      ],
      "metadata": {
        "id": "R7vvUi8FPD-A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV15ofn3f8Iy"
      },
      "source": [
        "def cos_similarity(a,b):\n",
        "  #keyedvectors class also provides some cosine similarity methods, here we use the standard formula\n",
        "  return np.dot(a,b)/( np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def make_matrix(vector_list,delta=0):\n",
        "  #matrix containing each cosine similarity between all pairs of sentences\n",
        "  matrix = np.zeros((len(vector_list), len(vector_list)))\n",
        "  for i in range(0,len(vector_list)):\n",
        "    for j in range(0, len(vector_list)):\n",
        "      matrix[i,j] = cos_similarity(vector_list[i], vector_list[j])\n",
        "\n",
        "  #make the matrix ergodic. By default delta = 0, so nothing changes because here the matrix is already ergodic\n",
        "  if delta !=0:\n",
        "    delta = delta/len(vector_list)\n",
        "  ergod_matrix = np.full(shape = (len(vector_list),len(vector_list)), fill_value = delta, dtype=float)\n",
        "  matrix = matrix + ergod_matrix\n",
        "\n",
        "  #make stochastic each vector\n",
        "  for j in range(len(matrix)):\n",
        "    matrix[j] = matrix[j].dot(1/np.sum(matrix[j]))\n",
        "\n",
        "  return matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuMZBU4-gFoC"
      },
      "source": [
        "Quatrième exercice : calcul du pageRank\n",
        "-------------\n",
        "Définir une fonction qui calcule le pageRank de la matrice stochastique et qui sélectionne les K phrases les plus pertinentes d'un texte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKoaAzsdge0-"
      },
      "source": [
        "def pagerank(G,K):\n",
        "\n",
        "  #1.page rank processing\n",
        "  X = np.full(len(G), 1/len(G), dtype=float)\n",
        "  iter = 0\n",
        "  while iter <10: #after 12 iterations, all values of X seem to be stabilized, setting to 15 just to be sure\n",
        "    X = X.dot(G)\n",
        "    iter +=1\n",
        "\n",
        "  #2. K best scores from vector\n",
        "  X = list(X)\n",
        "  best_list = []\n",
        "  for _ in range(K):\n",
        "    index = X.index(max(X))\n",
        "    best_list.append(index) #find best line's index\n",
        "    X.pop(index)        #remove best line, to find next best line\n",
        "\n",
        "  #3. return K best sentences\n",
        "  return best_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-OmHQghQ8aA"
      },
      "source": [
        "\n",
        "Cinquième exercice : Synthese et commentaires\n",
        "-------------\n",
        "Rassemblez les solutions obtenues dans les exercices précédents pour afficher le résumé extractif de chacun des textes du corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITet1NQ8RFiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7784770e-3def-4dcd-9510-7f8e16e36237"
      },
      "source": [
        "def summary(texte):\n",
        "\n",
        "  #text formating\n",
        "  texte = text_tokenize(texte)\n",
        "  sent_list = []\n",
        "  for sent in texte:\n",
        "    sent_list.append(sent_tokenize(sent))\n",
        "\n",
        "  #vector and matrix computing\n",
        "  vector_list = []\n",
        "  for index in range(len(sent_list)):\n",
        "    vector = vectorize_sent(sent_list[index], w2v)\n",
        "    if vector.all() == 0: #empty sentences\n",
        "      pass\n",
        "    else:\n",
        "      vector_list.append(vector)\n",
        "\n",
        "  matrix = make_matrix(vector_list)\n",
        "\n",
        "  #PageRank processing\n",
        "  best_list = pagerank(matrix, K=2)\n",
        "\n",
        "  #print summary\n",
        "  for elt in best_list:\n",
        "    print(\" \".join(sent_list[elt]).strip())\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "#generate summary for each author\n",
        "for index in range(len(corpus)):\n",
        "  print(writers[index])\n",
        "  summary(corpus[index])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corneille\n",
            "[[0.14442932 0.06669722 0.04960989 0.06237024 0.04874629 0.04063723\n",
            "  0.06894961 0.06811127 0.03027443 0.06648822 0.05825868 0.05323973\n",
            "  0.05741015 0.08629866 0.07495097 0.02352809]\n",
            " [0.07746661 0.16774987 0.061505   0.06026533 0.04764644 0.05400902\n",
            "  0.06814184 0.07353876 0.04126983 0.07648637 0.0525046  0.04241945\n",
            "  0.04825925 0.03160543 0.0663038  0.0308284 ]\n",
            " [0.04394978 0.04691287 0.12795103 0.07358917 0.05455233 0.073823\n",
            "  0.07564228 0.07130556 0.05167609 0.05560125 0.07775022 0.04197788\n",
            "  0.05997694 0.04411765 0.06398662 0.03718734]\n",
            " [0.05177513 0.04307294 0.06895556 0.11989448 0.05755832 0.0683242\n",
            "  0.07700047 0.07695259 0.0610687  0.04294445 0.0526728  0.04994948\n",
            "  0.07408467 0.05783054 0.06651363 0.03140205]\n",
            " [0.04988173 0.04197817 0.06301222 0.07095192 0.14779347 0.06210741\n",
            "  0.09794116 0.09130705 0.08017335 0.06304871 0.06463114 0.02087245\n",
            "  0.03419929 0.0331634  0.05139095 0.02754758]\n",
            " [0.03945094 0.04514324 0.08089776 0.07990316 0.05892191 0.14021311\n",
            "  0.07722942 0.0826336  0.05659482 0.05333483 0.07861448 0.03558908\n",
            "  0.04319724 0.02981022 0.06515968 0.03330651]\n",
            " [0.05789002 0.04925824 0.07168827 0.0778792  0.08035951 0.06679153\n",
            "  0.12126271 0.08376569 0.06975697 0.06222802 0.06541738 0.02459268\n",
            "  0.04307125 0.03136721 0.06874749 0.02592382]\n",
            " [0.05484459 0.05098288 0.06481116 0.07464389 0.07184876 0.06853908\n",
            "  0.0803358  0.11629745 0.06607761 0.06083357 0.06219656 0.04673264\n",
            "  0.05275086 0.04419482 0.06449559 0.02041473]\n",
            " [0.03460289 0.04061275 0.06667111 0.08408361 0.08955021 0.06663155\n",
            "  0.09496251 0.0937942  0.16507901 0.04738304 0.03027312 0.02094634\n",
            "  0.04971124 0.05273791 0.05746076 0.00549975]\n",
            " [0.06819625 0.06754497 0.06437421 0.05306144 0.06319636 0.05634993\n",
            "  0.07602035 0.07748974 0.04252088 0.14813959 0.09471784 0.02398251\n",
            "  0.02487664 0.02059841 0.07170067 0.04723022]\n",
            " [0.05493931 0.04262978 0.08276292 0.05983636 0.05956135 0.07636457\n",
            "  0.07347572 0.0728407  0.02497717 0.08708405 0.13620027 0.04373243\n",
            "  0.04445278 0.03134986 0.06552798 0.04426476]\n",
            " [0.06310304 0.04328851 0.0561625  0.07131839 0.0241762  0.04345082\n",
            "  0.03471749 0.06878914 0.02172131 0.02771362 0.05496616 0.1711866\n",
            "  0.10494834 0.10810311 0.07660319 0.0297516 ]\n",
            " [0.05877462 0.04253778 0.06931018 0.09136623 0.03421514 0.04555372\n",
            "  0.05251904 0.06706808 0.04452658 0.02483003 0.04825891 0.09064886\n",
            "  0.14786198 0.09054649 0.07094609 0.02103627]\n",
            " [0.09695915 0.03057311 0.05595111 0.07827051 0.03641195 0.03449981\n",
            "  0.04197481 0.06166536 0.05184074 0.02256331 0.03735061 0.10247279\n",
            "  0.09936998 0.1622707  0.07783227 0.00999379]\n",
            " [0.06655957 0.05069497 0.06414074 0.07115413 0.04459847 0.05960449\n",
            "  0.07271403 0.07112928 0.04464451 0.06207842 0.06170742 0.05739389\n",
            "  0.06154043 0.06151884 0.12825922 0.02226157]\n",
            " [0.04133731 0.04663367 0.07375004 0.06646143 0.0472976  0.06027697\n",
            "  0.05424786 0.0445435  0.008454   0.08090208 0.08246905 0.0441013\n",
            "  0.03610133 0.01562792 0.04404311 0.25375282]]\n",
            "cette pièce fut très bien accueillie et corneille enchaîna ensuite les succès durant quelques années mais la faveur grandissante des tragédies où dominait expression du sentiment amoureux de philippe quinault de son propre frère thomas et enfin de jean racine relégua ses créations au second plan\n",
            "il avait aussi donné dès 1634-35 une tragédie mythologique médée mais ce est qu'en 1640 qu'il se lança dans la voie de la tragédie historique — il fut le dernier des poètes dramatiques de sa génération à le faire — donnant ainsi ce que la postérité considéra comme ses chefs-d'œuvre : horace cinna polyeucte rodogune héraclius et nicomède\n",
            "\n",
            "\n",
            "\n",
            "Racine\n",
            "[[0.1769433  0.06837286 0.06712168 0.03023947 0.06385012 0.06113418\n",
            "  0.05514884 0.06134574 0.01436747 0.03003324 0.00487471 0.04678464\n",
            "  0.00376652 0.06282786 0.01488988 0.01751254 0.02917384 0.02279729\n",
            "  0.05973752 0.07686479 0.03221351]\n",
            " [0.05188992 0.1342868  0.05895604 0.04616039 0.04040846 0.05733236\n",
            "  0.08304049 0.06750578 0.03365459 0.07427067 0.0087501  0.04236435\n",
            "  0.02201632 0.04952474 0.02925143 0.03765218 0.03286017 0.01331792\n",
            "  0.04178309 0.04049495 0.03447924]\n",
            " [0.04724553 0.05467981 0.12454664 0.03957165 0.05701165 0.06245553\n",
            "  0.04823051 0.05649305 0.04051274 0.02065484 0.02793066 0.04315038\n",
            "  0.03384912 0.05629075 0.02959443 0.03818599 0.02424506 0.02879136\n",
            "  0.06193602 0.06103903 0.04358526]\n",
            " [0.03046439 0.06127574 0.05663755 0.17825938 0.05091163 0.05313944\n",
            "  0.09381808 0.07147193 0.07688371 0.01742796 0.02029896 0.03842708\n",
            "  0.03637594 0.05072193 0.04557542 0.01571095 0.01462156 0.00154898\n",
            "  0.03635718 0.03694456 0.01312762]\n",
            " [0.04284868 0.0357313  0.05435525 0.03391365 0.1187435  0.06260506\n",
            "  0.04590014 0.05596641 0.03082039 0.03344137 0.0198177  0.04884406\n",
            "  0.03435139 0.05408905 0.03941319 0.03547105 0.02873326 0.03473339\n",
            "  0.07076921 0.07326218 0.04618978]\n",
            " [0.03715182 0.04590887 0.05392238 0.03205492 0.05669303 0.10753012\n",
            "  0.05514957 0.03483203 0.03571738 0.05520168 0.02827711 0.05242699\n",
            "  0.03493906 0.05037138 0.03407641 0.04584437 0.03851421 0.04745317\n",
            "  0.05756247 0.05471417 0.04165886]\n",
            " [0.04020228 0.07976365 0.04995032 0.06788637 0.04986003 0.06615466\n",
            "  0.12898775 0.05696312 0.04038575 0.07790795 0.01562326 0.03362589\n",
            "  0.0172537  0.05349066 0.04151269 0.03161937 0.02645342 0.01844358\n",
            "  0.03970604 0.04235953 0.02184996]\n",
            " [0.04403965 0.06385593 0.05761779 0.05093036 0.05987027 0.04114739\n",
            "  0.05609691 0.12702629 0.05078224 0.02357703 0.01802331 0.04162452\n",
            "  0.02758294 0.05713495 0.04419631 0.03939938 0.02105074 0.02868356\n",
            "  0.05647688 0.05787616 0.03300741]\n",
            " [0.01475256 0.04553362 0.05909909 0.07836157 0.04715736 0.06034907\n",
            "  0.05688541 0.07263392 0.18168589 0.01184906 0.0479036  0.02500212\n",
            "  0.0007644  0.05753591 0.03733332 0.0226478  0.0167384  0.01615308\n",
            "  0.02909571 0.06706137 0.05145672]\n",
            " [0.02929423 0.09545485 0.02862226 0.01687362 0.04860581 0.08860049\n",
            "  0.1042431  0.03203387 0.01125581 0.17258935 0.01413557 0.05104808\n",
            "  0.0360923  0.04349539 0.02479171 0.04274736 0.0371245  0.03397557\n",
            "  0.04877348 0.03120274 0.00903992]\n",
            " [0.00581083 0.01374368 0.04730127 0.02401845 0.03520197 0.0554662\n",
            "  0.02554742 0.0299271  0.05561226 0.01727519 0.21092285 0.0617715\n",
            "  0.06455981 0.05381402 0.01436873 0.02487653 0.02096469 0.06281486\n",
            "  0.06923275 0.06429717 0.04247271]\n",
            " [0.03214623 0.03835554 0.04212244 0.02620872 0.05001069 0.05927693\n",
            "  0.03169468 0.03983974 0.0167308  0.03596055 0.03560619 0.12157966\n",
            "  0.04367376 0.07068559 0.03805133 0.05148056 0.04129268 0.05879861\n",
            "  0.07637932 0.05055345 0.03955254]\n",
            " [0.00403557 0.03108213 0.05152461 0.03868665 0.05484459 0.06159997\n",
            "  0.02535905 0.04116672 0.00079762 0.03964605 0.05802805 0.06810188\n",
            "  0.18958298 0.03484809 0.03669404 0.04529406 0.05068756 0.06079642\n",
            "  0.06162075 0.03410189 0.0115013 ]\n",
            " [0.0398895  0.04143137 0.05077447 0.03196567 0.05117288 0.05262525\n",
            "  0.04658755 0.05052992 0.03557609 0.02831194 0.02866238 0.06531462\n",
            "  0.02065    0.11234155 0.0450321  0.04784903 0.03066799 0.05376144\n",
            "  0.07316802 0.05779265 0.03589557]\n",
            " [0.01263905 0.03271682 0.03568908 0.03840039 0.04985275 0.04759716\n",
            "  0.04833812 0.05225763 0.03086262 0.02157495 0.0102318  0.0470074\n",
            "  0.02907057 0.06020591 0.15019564 0.08046602 0.05897446 0.07529887\n",
            "  0.06687128 0.03147439 0.0202751 ]\n",
            " [0.01333838 0.03778721 0.04131998 0.01187786 0.04025801 0.05745714\n",
            "  0.03303641 0.04180073 0.01679937 0.03337977 0.01589477 0.05706508\n",
            "  0.03219808 0.05740117 0.072201   0.1347684  0.05974212 0.08987017\n",
            "  0.0677438  0.03589653 0.05016403]\n",
            " [0.02942919 0.04367725 0.03474642 0.01464064 0.04319105 0.06393073\n",
            "  0.036606   0.02957961 0.01644415 0.03839418 0.01774123 0.06062209\n",
            "  0.04772224 0.04872632 0.07008506 0.07912456 0.17849201 0.07323308\n",
            "  0.05156526 0.01594758 0.00610135]\n",
            " [0.01842978 0.01418646 0.03306748 0.00124298 0.04184159 0.06312567\n",
            "  0.0204535  0.03230058 0.01271759 0.02815942 0.04260001 0.06917944\n",
            "  0.0458722  0.0684544  0.07171369 0.09538902 0.05868937 0.14304441\n",
            "  0.08580062 0.03154992 0.02218186]\n",
            " [0.03593598 0.03311949 0.05293315 0.02170972 0.06343822 0.05698044\n",
            "  0.03276609 0.04732532 0.01704607 0.03008058 0.03493852 0.06687\n",
            "  0.03459745 0.0693262  0.04739132 0.05350544 0.03075069 0.06384634\n",
            "  0.10644285 0.05916767 0.04182846]\n",
            " [0.05236394 0.03635018 0.05907648 0.02498257 0.07437192 0.06133505\n",
            "  0.03958601 0.05492184 0.04449285 0.02179304 0.03674576 0.05012206\n",
            "  0.02168294 0.06201134 0.02526033 0.03210727 0.01076998 0.02658682\n",
            "  0.06700496 0.12054217 0.07789251]\n",
            " [0.03011101 0.04246643 0.05788006 0.01218021 0.0643365  0.06407647\n",
            "  0.02801712 0.0429773  0.04684274 0.00866307 0.03330486 0.05380652\n",
            "  0.01003388 0.05284718 0.02232683 0.06156385 0.00565364 0.0256477\n",
            "  0.06499452 0.10687548 0.16539462]]\n",
            "les tragédies de racine se fondent sur la conjonction de la crainte et de la pitié les deux émotions fondamentales du théâtre antique ; la critique a souvent estimé que le dramaturge a ainsi cherché à associer la prédestination janséniste et le fatum antique\n",
            "la « tristesse majestueuse » de ces pièces épurées rompant avec héroïsme baroque fait la renommée du dramaturge et divise profondément le public français dont une partie défend la tragédie cornélienne\n",
            "\n",
            "\n",
            "\n",
            "Flaubert\n",
            "[[0.36053802 0.18749079 0.15727866 0.14262318 0.15206936]\n",
            " [0.1569239  0.30175899 0.19159597 0.19131053 0.1584106 ]\n",
            " [0.13091575 0.19054577 0.30010495 0.18757279 0.19086074]\n",
            " [0.1271475  0.20377338 0.2008933  0.32141695 0.14676887]\n",
            " [0.13877446 0.17272012 0.2092485  0.15023948 0.32901743]]\n",
            "flaubert se distingue par sa conception du métier écrivain et la modernité de sa poétique romanesque\n",
            "considéré avec victor hugo stendhal balzac et zola comme un des plus grands romanciers français du xixe siècle\n",
            "\n",
            "\n",
            "\n",
            "Balzac\n",
            "[[ 0.34970947  0.07709407  0.04132043  0.05170575  0.11826465  0.04899839\n",
            "   0.15613708  0.15677016]\n",
            " [ 0.08972018  0.40698328  0.0023737   0.13659373  0.09156023  0.11650205\n",
            "   0.09069076  0.06557606]\n",
            " [ 0.07980977  0.00393957  0.67545847 -0.01063909  0.04253142  0.0440086\n",
            "   0.10466782  0.06022344]\n",
            " [ 0.07347895  0.16679602 -0.00782775  0.49697151  0.07433474  0.08694492\n",
            "   0.05505878  0.05424283]\n",
            " [ 0.1290537   0.08585255  0.02402891  0.0570799   0.38161276  0.0851692\n",
            "   0.16869104  0.06851193]\n",
            " [ 0.05823061  0.11896901  0.02707795  0.07270924  0.09275483  0.41560128\n",
            "   0.16632645  0.04833063]\n",
            " [ 0.14959376  0.07466229  0.05191936  0.03712016  0.14810984  0.13409088\n",
            "   0.33505399  0.06944972]\n",
            " [ 0.19399448  0.06972718  0.03858338  0.04723286  0.07769207  0.05032444\n",
            "   0.0896993   0.43274629]]\n",
            "le bazar contient des boutiques mais également des ateliers et parfois des habitations\n",
            "le bazar en persan : بازار bāzār est un marché ou un ensemble de magasins où biens et services sont disponibles à la vente et à achat\n",
            "\n",
            "\n",
            "\n",
            "Zola\n",
            "[[0.27343987 0.11449115 0.09378767 0.06790733 0.0328254  0.10026049\n",
            "  0.08415015 0.0989753  0.01673181 0.11743082]\n",
            " [0.11216478 0.26788378 0.12291548 0.09727568 0.0401828  0.12597266\n",
            "  0.07254096 0.07592612 0.0297736  0.05536415]\n",
            " [0.09603422 0.12847015 0.2799897  0.08412337 0.05089741 0.14602636\n",
            "  0.0896161  0.01401379 0.05005558 0.06077333]\n",
            " [0.08050425 0.1177123  0.09739543 0.32416341 0.08410593 0.14020364\n",
            "  0.07232604 0.03030307 0.01309078 0.04019515]\n",
            " [0.04204742 0.05253937 0.06367144 0.09087694 0.35026046 0.11674886\n",
            "  0.05067106 0.1419118  0.02552428 0.06574838]\n",
            " [0.08183125 0.10494966 0.11639662 0.09652648 0.07438968 0.22317789\n",
            "  0.11146778 0.07040651 0.07192795 0.04892617]\n",
            " [0.09212142 0.08105959 0.09581015 0.06678797 0.04330483 0.14950845\n",
            "  0.29934192 0.05974271 0.04780563 0.06451733]\n",
            " [0.1194308  0.09351819 0.01651448 0.03084423 0.13368379 0.10409095\n",
            "  0.06585197 0.32995243 0.05882505 0.04728812]\n",
            " [0.02677031 0.04862471 0.07821366 0.01766744 0.03188119 0.14099987\n",
            "  0.06986886 0.07799794 0.43749407 0.07048195]\n",
            " [0.14788116 0.07116637 0.07474181 0.04269748 0.0646378  0.07548882\n",
            "  0.07421666 0.04935069 0.05547513 0.34434406]]\n",
            "sur le plan littéraire il est principalement connu pour les rougon-macquart une fresque romanesque en vingt volumes dépeignant la société française sous le second empire qui met en scène la trajectoire de la famille des rougon-macquart à travers ses différentes générations et dont chacun des représentants une époque et une génération particulière fait objet un roman\n",
            "considéré comme le chef de file du naturalisme est un des romanciers français les plus populaires les plus publiés traduits et commentés dans le monde entier\n",
            "\n",
            "\n",
            "\n",
            "Baudelaire\n",
            "[[0.40894761 0.17383997 0.17733663 0.11534885 0.12452695]\n",
            " [0.14803797 0.34825004 0.16364115 0.19737847 0.14269237]\n",
            " [0.15832612 0.17156281 0.36510839 0.15849189 0.14651079]\n",
            " [0.10277003 0.2065045  0.15816345 0.36435179 0.16821024]\n",
            " [0.11815389 0.15898712 0.15570414 0.17913642 0.38801844]]\n",
            "« dante une époque déchue » selon les mots de barbey aurevilly « tourné vers le classicisme nourri de romantisme » à la croisée entre le parnasse et le symbolisme chantre de la « modernité » il occupe une place considérable parmi les poètes français pour un recueil certes bref au regard de œuvre de son contemporain victor hugo baudelaire ouvrit à son éditeur de sa crainte que son volume ne ressemblât trop à « une plaquette » mais qu'il aura façonné sa vie durant : les fleurs du mal\n",
            "au cœur des débats sur la fonction de la littérature de son époque baudelaire détache la poésie de la morale la proclame tout entière destinée au beau et non à la vérité\n",
            "\n",
            "\n",
            "\n",
            "Rimbaud\n",
            "[[ 0.20663916  0.05777042  0.14064664  0.1071059   0.01968796  0.04702598\n",
            "   0.10837255  0.07371495  0.03439107  0.02537293  0.02352417  0.02059805\n",
            "   0.05462296  0.08052726]\n",
            " [ 0.05438043  0.19451347  0.07397891  0.09810592  0.04072161  0.07427856\n",
            "   0.08751942  0.08116264  0.02892891  0.00699198  0.03742391  0.06968347\n",
            "   0.03847332  0.11383746]\n",
            " [ 0.12314867  0.06881311  0.180931    0.105046    0.01224472  0.05548418\n",
            "   0.09238911  0.10927054  0.04331425  0.00962427  0.00641478  0.0545502\n",
            "   0.05650908  0.08226007]\n",
            " [ 0.09405623  0.09152344  0.10535457  0.18146247  0.02137645  0.06445137\n",
            "   0.09111693  0.06131127  0.04074825  0.02906216  0.02716403  0.05813048\n",
            "   0.04716035  0.087082  ]\n",
            " [ 0.02308041  0.05071432  0.01639424  0.02853673  0.2422453   0.16598348\n",
            "   0.0590421   0.03766185  0.03813862  0.00134663  0.09289964  0.0441239\n",
            "   0.16121245  0.03862032]\n",
            " [ 0.04079535  0.06845407  0.05497202  0.06366941  0.12282731  0.17926085\n",
            "   0.06565124  0.06173114  0.05208437 -0.00639987  0.07709082  0.02918173\n",
            "   0.12983316  0.0608484 ]\n",
            " [ 0.09232731  0.07920968  0.08989413  0.08839664  0.04290718  0.06447346\n",
            "   0.17604492  0.07144387  0.06039338  0.02747965  0.03197164  0.04342737\n",
            "   0.03421488  0.09781589]\n",
            " [ 0.06707562  0.07845638  0.11355648  0.06352948  0.02923264  0.06475012\n",
            "   0.07630681  0.18802768  0.06274123  0.01493399  0.02791802  0.06717177\n",
            "   0.04711508  0.09918469]\n",
            " [ 0.04039664  0.03609895  0.05810721  0.05450474  0.03821394  0.07052355\n",
            "   0.08326798  0.08099225  0.24272371  0.06249684  0.04393171  0.08821642\n",
            "   0.02431811  0.07620794]\n",
            " [ 0.05790645  0.01695194  0.02508557  0.07552838  0.00262158 -0.01683661\n",
            "   0.07361337  0.03745615  0.1214269   0.47159482  0.00218019  0.07197418\n",
            "  -0.0249267   0.08542378]\n",
            " [ 0.03225152  0.05450646  0.01004425  0.04240879  0.10864434  0.1218332\n",
            "   0.05145053  0.04206407  0.05127603  0.00130971  0.28330122  0.01734144\n",
            "   0.10844857  0.07511989]\n",
            " [ 0.02414679  0.08678127  0.07303471  0.07760023  0.04412294  0.03943406\n",
            "   0.05975661  0.08653882  0.08804064  0.03697035  0.014828    0.24224004\n",
            "   0.03591183  0.0905937 ]\n",
            " [ 0.0576682   0.0431503   0.06813639  0.05669752  0.14518343  0.15800615\n",
            "   0.04239996  0.05466533  0.02185704 -0.01153107  0.08351206  0.03234189\n",
            "   0.21815935  0.02975345]\n",
            " [ 0.06763142  0.10156726  0.07890315  0.08328366  0.02766806  0.05890904\n",
            "   0.09642822  0.09154637  0.05448868  0.03143605  0.04601768  0.06490383\n",
            "   0.0236691   0.17354745]]\n",
            "de cette seconde vie exotique les seuls écrits connus consistent en près de 180 lettres correspondance familiale et professionnelle et quelques descriptions géographiques\n",
            "développant déjà une franche originalité dans approche de thèmes classiques « le dormeur du val » « vénus anadyomène » il cherche à dépasser ces influences en développant ses propres conceptions théoriques déclarant que le poète doit se faire « voyant » est-à-dire chercher et décrire inconnu par delà les perceptions humaines usuelles quitte à y sacrifier sa propre intégrité mentale ou physique\n",
            "\n",
            "\n",
            "\n",
            "Verlaine\n",
            "[[ 0.20995236  0.09874687  0.03214319  0.12600525  0.11311849  0.09784968\n",
            "   0.09972771  0.06035992  0.10335232  0.0587442 ]\n",
            " [ 0.11599273  0.24661994  0.01333761  0.07428686  0.11570446  0.13733486\n",
            "   0.08835452  0.00262152  0.10127078  0.10447672]\n",
            " [ 0.06522116  0.02303934  0.42601046  0.09220751  0.10769982  0.11722665\n",
            "   0.09956076  0.03428084  0.03901465 -0.00426119]\n",
            " [ 0.14136159  0.07094917  0.05098114  0.23553937  0.11110959  0.10518113\n",
            "   0.10690963  0.03915482  0.10204434  0.03676923]\n",
            " [ 0.12482013  0.10869103  0.05856882  0.10928482  0.23167106  0.11293687\n",
            "   0.09046111  0.02794772  0.0890334   0.04658505]\n",
            " [ 0.10074362  0.12037367  0.05948193  0.09652798  0.10537628  0.21616178\n",
            "   0.09391536  0.02672395  0.11851366  0.06218176]\n",
            " [ 0.12067704  0.0910186   0.05937416  0.11531422  0.09920182  0.11037921\n",
            "   0.25405606  0.04453695  0.07983592  0.02560601]\n",
            " [ 0.1411663   0.0052195   0.03951248  0.0816253   0.0592349   0.06070506\n",
            "   0.08607837  0.49102444  0.02954417  0.0058895 ]\n",
            " [ 0.12069275  0.10067872  0.02245377  0.1062202   0.09422429  0.13442226\n",
            "   0.07704607  0.01475198  0.24517813  0.08433183]\n",
            " [ 0.10010557  0.15156725 -0.0035787   0.05585153  0.07194304  0.10291962\n",
            "   0.03606006  0.0042913   0.12306196  0.35777835]]\n",
            "paul verlaine est un écrivain et poète français né à metz moselle le 30 mars 1844 et mort à paris le 8 janvier 1896\n",
            "sa vie est bouleversée quand il rencontre arthur rimbaud en septembre 1871\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "931rMLOYkUFQ"
      },
      "source": [
        "Analysez votre démarche et indiquez quels paramètres ont un impact sur le résumé produit au final, comme par exemple:\n",
        "*  Segmentation et mots inconnus\n",
        "*  Quels mots inclure dans la vectorisation de la phrase ?\n",
        "*  Paramètre delta de lissage\n",
        "* ... (autres)\n",
        "\n",
        "A votre avis, comment pourrait-on mesurer la qualité d'un résumé ?\n",
        "\n",
        "La méthode proposée est-elle convaincante ? A votre avis, comment pourrait-on l'améliorer ?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**note:** ici Balzac renvoie à la page wikipedia Bazar, l'algorithme fonctionne ici aussi. Ce problème vient du module wikipedia lui même qui renvoie à la mauvaise page.\n",
        "\n",
        "**réglage des paramètres**:\n",
        "\n",
        "  * les règles de segmentation sont très simples mais semblent fonctionner\n",
        "  * les mots inconnus sont simplement ignorés par l'algorithme car aucun vecteur ne lui correspond, la plupart sont des nombres (dates) ou bien des mots rares (\"racinien\", \"épurement\"), noms propres (\"valincour\", \"ménélik\"), ou\n",
        "  * Mots à exlure: A l'origine, l'idée était de ne prendre en compte que les mots plus longs que 2 caractères, on évitait ainsi de considérer la plupart des déterminants et pronoms. Cette solution a été choisie car w2v ne donne pas d'information sur la classe grammaticale des mots qu'il vectorise. Toutefois, après discussion avec Mathilde Ducos, elle m'a fait remarquer que les mots de w2v.vocab semblaient être triés par ordre de fréquence. Ainsi, on choisi de ne plus considérer les 65 mots les plus fréquents, la plupart n'apportant pas d'information particulière. Ces mots sont stockés dans very_frequent_words, créée au début du programme. L'utilisation de cette méthode ne change que très peu, car la fonction de base fonctionnait assez bien, mais cette approche semble bien plus stable, voilà pourquoi elle a été préférée.\n",
        "\n",
        "  * Il n'y a pas eu besoin de définir de paramètre delta de lissage, car:\n",
        "    * chaque vecteur de phrase est de dimension 200, dont toutes les valeurs sont différentes de 0. Dans le cas d'une phrase ne contenant aucun mot reconnu (phrase vide, en langue étrangère), on considère qu'on ne peut pas la traiter et on l'exclue donc du corpus.\n",
        "    * le calcul de similarité entre deux vecteurs renvoie une valeur entre 0 et 1 à partir du moment où les deux vecteurs partagent au moins une dimension de valeur non nulle. par conséquent aucun de nos calculs de similarité n'a pu résulter à 0.\n",
        "    * Ainsi, on déduit que la matrice obtenue est déjà ergodique, et qu'il n'est pas nécessaire d'appliquer une matrice de lissage.\n",
        "    *   **note sur le paramètre delta**: étant donné que la matrice est déjà ergodique, peu importe la valeur de delta, on obtiendra toujours la même matrice finale après l'avoir rendue stochastique. Aussi, il est possible que l'application d'une matrice de lissage ne soit pas nécessaire, car nous n'avons pas forcément besoin de rendre la matrice ergodique. Les besoins de Google lors de l'implémentation de PageRank les ont poussé à rendre leurs matrices ergodiques, mais dans ce cas précis il ne semble même pas nécessaire de faire de même.\n",
        "\n",
        "**calcul de la qualité d'un résumé:** Afin de déterminer la qualité d'un résumé de texte issu de l'introduction d'un article de wikipedia on peut définir plusieurs paramètres:\n",
        "  * la présence de caractéristiques essentielles:\n",
        "    * sujet (ici le titre de l'article),\n",
        "    * date de naissance et de mort si le sujet est un être humain\n",
        "    * profession exercée\n",
        "    * il est possible de détecter  ces informations par le biais de regex pour les dates, de présence de mots clés (\"né.e\",\"mort.e\",noms de professions exercées)\n",
        "  * la longueur du résumé, car un résumé trop long n'est plus forcément un résumé. Un résumé trop court pourra passer sous silence certaines informations importantes. Ici on joue sur la longueur du résumé avec le paramètre K de pagerank()\n",
        "\n",
        "\n",
        "**La méthode implémentée ici** donne un résultat qui n'est pas mauvais, on extrait bien K phrases qui ont le meilleur score selon l'algorithme PageRank, un algorithme qui a fait ses preuves.\n",
        " Toutefois nous n'avons aucun contrôle sur les vecteurs utilisés par w2v, ils sont par ailleurs assez compliqué à interpréter, il suffit de faire quelques tests pour s'en rendre compte:\n",
        "* la similarité entre les vecteurs de \"poète\" et \"poètes\" est de 0.68197\n",
        "* la similarité entre les vecteurs de \"poète\" et \"écrivain\" est de 0.73381\n",
        "\n",
        "Afin d'améliorer la qualité de nos résumés on pourrait proposer plusieurs solutions:\n",
        "* Sur wikipedia la première phrase est toujours une phrase qui résume très brièvement les informations principales à savoir sur le sujet en question. On pourrait donc obligatoirement mettre cette ligne en tête de notre résumé.\n",
        "* Afin d'obtenir les phrases qui centralisent au mieux les informations du texte, on pourrait calculer la similarité de chaque vecteur de phrase avec un vecteur général du texte.\n",
        "* On pourrait aussi calculer la proximité de chaque phrase avec une phrase référence, ou bien un vecteur patron.\n",
        "\n",
        "Ces deux dernières approches nous éloignent de l'approche de PageRank, mais peut être qu'elles pourraient améliorer la qualité des résumés produits.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t3PX9XqLGOyB"
      }
    }
  ]
}