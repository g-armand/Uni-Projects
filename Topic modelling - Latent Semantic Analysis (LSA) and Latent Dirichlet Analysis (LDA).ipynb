{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Goal of the TP\n",
        "\n",
        "In this TP, we use two topic modelling methods.\n",
        "The goal of topic modelling is to describe a set of documents in terms of a set of topics (unknown a priori) such that each document is more or less strongly correlated with each topic and that each topic is more or less strongly correlated with each element of the vocabulary (\"form\", or \"word\").\n",
        "More specifically, we use here the scikit-learn library to train and then analyse a Latent Semantic Analysis (LSA) model and a Latent Dirichlet Allocation (LDA) model.\n",
        "\n",
        "We use a \"real\" dataset consisting of the English subtitles of the *Game of Thrones* TV series but also an artificial dataset this is particularly well suited to topic modelling.\n",
        "This artificial dataset is useful to test the code and have an idea of the results of the different methods in an ideal case."
      ],
      "metadata": {
        "id": "FPXuQTE_mx_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_controlled_dataset = True\n",
        "#use_controlled_dataset = False"
      ],
      "metadata": {
        "id": "s_QZ8czqmxgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of a controlled dataset\n",
        "\n",
        "In order to more easily test the topic modelling methods that we are about to implement, we here create an artificial dataset consisting of \"texts\" that we know correspond to a mixture of well-defined \"topics\".\n",
        "A topic modelling algorithm should be able to recognise the topics in these documents fairly well.\n",
        "\n",
        "The topics are coded as the integer from 0 to `(n_artificial_topics-1)` (i.e. 15), or, equivalently, the letters from \"A\" (for 0) to \"P\" (for 15).\n",
        "Each topic is associated a small set of forms, all obtained by repeating the letter corresponding to the topic (e.g. \"B\" or \"BBB\" for topic \"B\").\n",
        "Each of the `n_docs=40` documents is assigned a topic `main_topic` and consists of `doc_length=2000` tokens. These tokens are not neccessarily occurrences of a form associated with `main_topic`, but each is an occurrence of a form associated with a topic selected randomly with a probability decreasing with the distance between the topic and `main_topic`.\n",
        "\n",
        "For instance, the eighth document (named \"doc_C_7\") is assigned topic \"C\"; it is therefore likely to contain a lot of tokens in \"C\", a bit less of tokens in \"B\" or \"D\", a bit less of tokens in \"A\" or \"E\", etc.\n",
        "\n",
        "The dataset is encoded as a list `dataset` that contains each document as a dictionary.\n",
        "This dictionary contains:\n",
        "*   an identifier (*str_id*) of the form \"doc_x_yy\" for \"x\" is the letter of its main topic and \"yy\" is a unique numerical identifier for the document;\n",
        "*   a string (*raw_text*), the content of the document."
      ],
      "metadata": {
        "id": "NTPl31RdnLi9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xSgbsDvmvGF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "if(use_controlled_dataset):\n",
        "  n_docs = 40 # Number of documents.\n",
        "  doc_length = 2000 # Length of each document.\n",
        "  n_topics = 16 # Number of topics.\n",
        "\n",
        "  # Returns a character.\n",
        "  # topic: int\n",
        "  def topic_to_letter(topic):\n",
        "    return chr(65 + topic) # 65 corresponds to \"A\".\n",
        "\n",
        "  dataset = []\n",
        "  for i in range(n_docs): # For each document.\n",
        "    main_topic = int(i  * n_topics / n_docs) # Main topic of the document.\n",
        "    print(f\"{topic_to_letter(main_topic)} ({main_topic})\", end=\", \", flush=True)\n",
        "\n",
        "    tokens = [] # list[str]\n",
        "    for j in range(doc_length): # For each token.\n",
        "      # Selects a topic for the token, likely to be `main_topic`, or one very near.\n",
        "      topic = main_topic + int(np.random.randn() * 1.5)\n",
        "      while(topic < 0): topic += n_topics\n",
        "      topic = topic % n_topics\n",
        "\n",
        "      # Generates the token.\n",
        "      #token_length = np.floor(1 + 8 * np.random.uniform()) # Between 1 and 8.\n",
        "      token_length = np.floor(9 - 8 * np.power(np.random.uniform(), 2)) # Between 1 and 8; more likely to be large than small.\n",
        "      token = topic_to_letter(topic) * int(token_length)\n",
        "\n",
        "      tokens.append(token)\n",
        "\n",
        "    dataset.append({\"str_id\": f\"doc_{topic_to_letter(main_topic)}_{i:02}\", \"raw_text\": \" \".join(tokens)})\n",
        "else:\n",
        "  print(\"[controlled dataset not in use]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_controlled_dataset):\n",
        "  print(dataset[-1][\"str_id\"]) # Name of the last artificial document.\n",
        "else:\n",
        "  print(\"[controlled dataset not in use]\")"
      ],
      "metadata": {
        "id": "RBvtPkQpnasY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_controlled_dataset):\n",
        "  print(dataset[7][\"str_id\"]) # Name of the 8th artificial document.\n",
        "  print(dataset[7][\"raw_text\"]) # Text of the 8th artificial document. It should be composed mainly of tokens in C, then tokens in B or D, then tokens in A or E, etc.\n",
        "else:\n",
        "  print(\"[controlled dataset not in use]\")"
      ],
      "metadata": {
        "id": "FW4a5I3qnbjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the Game of Thrones dataset"
      ],
      "metadata": {
        "id": "2E37cgXSndeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and extracting the dataset"
      ],
      "metadata": {
        "id": "FKsTUotfneuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib # To download files.\n",
        "import zipfile # To unzip files.\n",
        "\n",
        "zip_url = \"https://moodle.u-paris.fr/mod/resource/view.php?id=781987\"\n",
        "zip_filename = \"data.zip\"\n",
        "data_dirname = zip_filename.split(\".\")[0] # Name of the directory in which the dataset is/will be.\n",
        "\n",
        "if(not use_controlled_dataset):\n",
        "  if(os.path.isdir(data_dirname)):\n",
        "    print(\"Dataset found.\")\n",
        "  else:\n",
        "    # Downloads the dataset.\n",
        "    tmp = urllib.request.urlretrieve(zip_url)\n",
        "    filename = tmp[0]\n",
        "    print(f\"Dataset downloaded to '{filename}'.\")\n",
        "\n",
        "    # Extracts the dataset.\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "      zip_ref.extractall(\".\", )\n",
        "    assert os.path.isdir(data_dirname)\n",
        "    print(f\"Dataset extracted to '{data_dirname}'.\")\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "NK3RZ2GTngQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  print(dataset[-1][\"str_id\"]) # Name of the last document.\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "xyxdfgeAnhPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the Game of Thrones dataset\n",
        "\n",
        "The dataset contains one SRT file per episodes of the Game of Thrones series, grouped by season (one subdirectory per season).\n",
        "\n",
        "(The syntax of STR files is described here: https://docs.fileformat.com/video/srt/)\n",
        "\n",
        "As for the artificial dataset, this dataset is encoded as a list `dataset` that contains each document as a dictionary.\n",
        "This dictionary contains:\n",
        "*   an identifier (*str_id*) of the form \"doc_x_yy\" for \"x\" is the letter of its main topic and \"yy\" is a unique numerical identifier for the document;\n",
        "*   a string (*raw_text*), the concatenation of the subtitles of the corresponding episode."
      ],
      "metadata": {
        "id": "cPm0H6kfnlOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  dataset = [] # This will be a list of dictionaries.\n",
        "\n",
        "  # file_path: str\n",
        "  # Returns a string.\n",
        "  def read_srt_file(file_path):\n",
        "      lines = []\n",
        "      with open(file_path) as f:\n",
        "          c = True\n",
        "          while(c):\n",
        "              s = f.readline() # This should be the end of the file, an empty line or a number.\n",
        "              if(s == \"\"): c = False # The end of the file has been reached.\n",
        "              if(s.strip() == \"\"): continue # End of the file or empty line.\n",
        "\n",
        "              f.readline() # We can throw away the timing that follows.\n",
        "\n",
        "              # All the next non-empty lines are character lines.\n",
        "              s = f.readline().strip()\n",
        "              while(s != \"\"):\n",
        "                  if(not s.startswith(\"#\")): lines.append(s) # Ignores lines that start with '#'.\n",
        "                  s = f.readline().strip()\n",
        "\n",
        "      return ' '.join(lines)\n",
        "\n",
        "  for path, dirs, files in os.walk(data_dirname): # Iterates through every subdirectories.\n",
        "      path_parts = path.split(os.path.sep)\n",
        "      if(len(path_parts) == 1): # If we are in the root directory.\n",
        "        continue # There is no subtitle file in the root directory, only some 'info.txt' file.\n",
        "      season = path_parts[1] # 'Sxx' where 'xx' is the season number.\n",
        "      #print(season)\n",
        "\n",
        "      for file in files:\n",
        "          file_parts = file.replace(' ', '.').split(\".\") # Most files have a name that starts with 'Game.of.Thrones.SxxEyy.' where 'xx' is the season number and 'yy' the episode number, but some filenames use ' ' instead of '.'.\n",
        "          str_id = file_parts[3] # 'SxxEyy' where 'yy' is the episode number.\n",
        "          print(str_id, end= \", \")\n",
        "\n",
        "          file_path = os.path.join(path, file)\n",
        "          #print(file_path)\n",
        "\n",
        "          dataset.append({\"str_id\": str_id, \"raw_text\": read_srt_file(file_path)})\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "fjZHR4h9oanI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  dataset = sorted(dataset, key=(lambda x: x[\"str_id\"])) # Sorts the episode chronologically (via their season and episode number).\n",
        "\n",
        "  for episode in dataset: print(episode[\"str_id\"], end=\", \")\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "gd0C4di5oa2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  print(f'{dataset[0][\"str_id\"]}:')\n",
        "  print(dataset[0][\"raw_text\"][:200]) # Beginning of the first episode.\n",
        "  print(\"[…]\")\n",
        "  print(dataset[0][\"raw_text\"][-200:]) # End of the first episode.\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "zUyb8u1nodIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "In this subsection, the dictionnary for each document is enriched, most notably with *processed_tokens*, a list of tokens that will later be converted into a bag-of-words vector.\n",
        "\n",
        "This list is obtained from the subtitles through a few steps: (i) normalisation/simplification of the text, (ii) tokenisation, (iii) filtering of stop words (i.e. words that are mostly irrelevant to the problem; in our case — topic modelling —, stop words are typically determiners, coordinating or subordinating conjunctions, but others are possible.)."
      ],
      "metadata": {
        "id": "qgWErcR7ogUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bloc of code defines, if necessary, the set of stop words (`stopwords`) used afterwards."
      ],
      "metadata": {
        "id": "K6wAiwBhoh9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_controlled_dataset):\n",
        "  filter_stopwords = False\n",
        "else:\n",
        "  filter_stopwords = True\n",
        "  #filter_stopwords = False\n",
        "\n",
        "language = \"english\"\n",
        "#language = \"french\"\n",
        "\n",
        "import nltk\n",
        "\n",
        "if(filter_stopwords):\n",
        "  try:\n",
        "      print(f\"NLTK stop words: {nltk.corpus.stopwords.words(language)}\") # This might fail if \"stopwords\" is missing.\n",
        "  except:\n",
        "      nltk.download('stopwords')\n",
        "      print(f\"NLTK stop words: {nltk.corpus.stopwords.words(language)}\")\n",
        "\n",
        "  stopwords = set()\n",
        "  stopwords.update(set(nltk.corpus.stopwords.words(language)))\n",
        "\n",
        "  # Additional stop words.\n",
        "  if(language == \"english\"): stopwords.update({})\n",
        "  elif(language == \"french\"): stopwords.update({\"a\", \"si\", \"plus\", \"fait\", \"faire\", \"ça\", \"tout\", \"tous\", \"toute\", \"toutes\", \"ce\", \"celui\", \"ceux\", \"celle\", \"celles\", \"son\", \"sa\", \"ses\", \"leur\", \"leurs\", \"tu\", \"dit\", \"oui\", \"non\", \"si\", \"alors\", \"ne\", \"être\", \"avoir\", \"faut\", \"veux\", \"i\", \"ici\", \"là\", \"où\", \"quand\", \"veut\", \"peut\", \"il\", \"ils\", \"elle\", \"elles\", \"mais\", \"ou\", \"et\", \"donc\", \"car\"})\n",
        "\n",
        "  print(f\"Stop words used: {stopwords}\")"
      ],
      "metadata": {
        "id": "cEeDRJOzoitl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bloc of code makes sure that the tokeniser (`nltk.word_tokenize`) is available."
      ],
      "metadata": {
        "id": "ssahyfllolqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(nltk.word_tokenize(\"NLTK tokeniser ready.\")) # This might fail if \"punkt\" is missing.\n",
        "except:\n",
        "    nltk.download('punkt') # Necessary to use nltk.word_tokenize.\n",
        "    print(nltk.word_tokenize(\"NLTK tokeniser ready.\"))"
      ],
      "metadata": {
        "id": "CUqsVfRoon3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bloc of code defines, if necessary, a stemmer (stemmer)."
      ],
      "metadata": {
        "id": "HxJQE51Oop8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stem_words = True\n",
        "stem_words = False\n",
        "\n",
        "if(stem_words):\n",
        "    stemmer = nltk.stem.snowball.SnowballStemmer(language) # https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.SnowballStemmer\n",
        "\n",
        "    for w in [\"running\", \"mangeront\"]: # One English word and one French word.\n",
        "      print(f\"{w} -> {stemmer.stem(w)}\")"
      ],
      "metadata": {
        "id": "zh5dBu2ooqx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bloc of code defines the preprocessing function.\n",
        "This is the function that implements the three steps mentioned above (normalisation/simplification, tokenisation, filtering).\n",
        "\n",
        "When using the controlled dataset, only the tokenisation step is relevant.\n",
        "You can implement the other two later.\n",
        "When using the Game of Throne dataset, the normalisation/simplification process should at least convert the text to lower case and remove punctuation marks and the HTML tags that subtitles sometimes contain.\n",
        "Additional operations are possible, both in the normalisation/simplification step and after the tokenisation step."
      ],
      "metadata": {
        "id": "pmzuRDCcosGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # To use regular expressions (regexes). https://docs.python.org/3/library/re.html https://docs.python.org/3/howto/regex.html\n",
        "\n",
        "# Returns a pair composed of (i) a string and a list of tokens.\n",
        "# text: str\n",
        "def preprocess(text):\n",
        "    # (i) Normalisation/simplification step\n",
        "    tmp = text\n",
        "    tmp = re.sub('<.*?>', '', tmp) # Removes anything that looks like an HTML tag.\n",
        "    ## TODO\n",
        "    processed_text = ()\n",
        "\n",
        "    # (ii) Tokenisation step\n",
        "    ## TODO\n",
        "    tokens = ()\n",
        "\n",
        "    # (iii) Filtering step\n",
        "    ## TODO\n",
        "    tokens = ()\n",
        "\n",
        "    return (processed_text, tokens)\n",
        "\n",
        "# Test\n",
        "print(preprocess(\"Hello Jon Snow, how are you?  I'm fine thanks,   what about you ? Please count to 3. 1, 2, 3. Good.\"))"
      ],
      "metadata": {
        "id": "_9oFW5DTowxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bloc of code actually processes the documents using the function defined above."
      ],
      "metadata": {
        "id": "OjmHh5QhpAU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for document in dataset:\n",
        "    print(document[\"str_id\"], end=\", \", flush=True)\n",
        "\n",
        "    (text, tokens) = preprocess(document[\"raw_text\"])\n",
        "    document[\"processed_text\"] = text\n",
        "    document[\"processed_tokens\"] = tokens"
      ],
      "metadata": {
        "id": "aOCTA615pBTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{dataset[0][\"str_id\"]}:')\n",
        "print(dataset[0][\"processed_tokens\"][:20])\n",
        "print(\"[…]\")\n",
        "print(dataset[0][\"processed_tokens\"][-20:])"
      ],
      "metadata": {
        "id": "wgY97Fk5pCw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of the document-form matrix\n",
        "\n",
        "The aim of this section is to create a document-form matrix `matrix` that indicates, for a selected set of forms (e.g. words), for each document, the number of occurrences of each of these forms in the document.\n",
        "\n",
        "This is done through three steps: (i) counting of the frequency of all forms in each document, (ii) creation of a vocabulary that includes or not forms based on their document-frequency (i.e. the proposition of documents in which they occur at least once; the goal being of filtering too rare and too common forms), (iii) creation of the document-form matrix restricted to the vocabulary thus determined."
      ],
      "metadata": {
        "id": "o5xn3KUKpGu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting of the frequency of all forms in each document\n",
        "\n",
        "This bloc of code counts, (i) for each form, the number of documents in which it appears, and (ii) for each document, the number of occurrences (i.e. frequency) of each form."
      ],
      "metadata": {
        "id": "12PmOnYopJpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter # https://docs.pythhttps://www.gutenberg.org/cache/epub/6838/pg6838.txton.org/3/library/collections.html#collections.Counter\n",
        "\n",
        "document_form_counts = Counter() # Form each form, the number of documents it occurs in.\n",
        "for document in dataset:\n",
        "    print(document[\"str_id\"], end=\", \", flush=True)\n",
        "\n",
        "    document[\"counts\"] = Counter(document[\"processed_tokens\"]) # For each form, the number of its occurences in the document.\n",
        "    document_form_counts.update(set(document[\"processed_tokens\"]))\n",
        "print()\n",
        "\n",
        "print(document_form_counts)"
      ],
      "metadata": {
        "id": "kW2EtET0pMJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of a vocabulary\n",
        "\n",
        "We here define the vocabulary used afterwards.\n",
        "This vocabulary contains the forms with a document-frequency above a given lower bound `min_df` and below a given higher bound `max_df`.\n",
        "This vocabulary is encoded as a dictionary `form2id`, that associates an integer identifier to each form, and, conversely, a list `id2form`, that associates its form to each identifier."
      ],
      "metadata": {
        "id": "ySVdbuJSpRLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to play with these values. Some probably yield better results than others, depending on the data.\n",
        "max_df = 0.9 # Upper limit for the document frequency of a form.\n",
        "min_df = 0.06 # Lower limit for the document frequency of a form.\n",
        "\n",
        "# TODO\n",
        "form2id = () # From form (str) to id (int).\n",
        "id2form = () # From id (int) to form (str).\n",
        "\n",
        "\n",
        "print(f\"Number of forms: {len(id2form)}\")"
      ],
      "metadata": {
        "id": "Z8a5IzHtpSc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of the document-form matrix\n",
        "\n",
        "We here define a document-form matrix as a bidimensional Numpy array.\n",
        "Lines are indexed by documents, columns by forms."
      ],
      "metadata": {
        "id": "hA2KCCf4pdz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# TODO\n",
        "matrix = ()\n",
        "\n",
        "\n",
        "print(matrix.shape)\n",
        "print(matrix)\n",
        "\n",
        "doc_lengths = matrix.sum(axis=1) # For each document, its length.\n",
        "term_frequency = matrix.sum(axis=0) # For each form, its frequency (count)."
      ],
      "metadata": {
        "id": "oVUAetcDphFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Semantic Analysis (LSA)"
      ],
      "metadata": {
        "id": "cLnMngRRpq62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning a model"
      ],
      "metadata": {
        "id": "Bmg8kVxWpsLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "if(use_controlled_dataset): lsa_n_topics = n_topics\n",
        "else: lsa_n_topics = 32 # Find a good value by trial and error.\n",
        "\n",
        "assert lsa_n_topics <= len(dataset) # With LSA, their cannot be more topics than documents.\n",
        "lsa_model = TruncatedSVD(n_components=lsa_n_topics, n_iter=10)\n",
        "\n",
        "print(\"Fitting the model…\", end=\"\", flush=True)\n",
        "lsa_model.fit(X=matrix)\n",
        "print(\" Done!\")"
      ],
      "metadata": {
        "id": "jWhVM8ZzptZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "hiwp2RB5pxNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_form_corr = lsa_model.components_ # Contains a description of each topics in terms of the forms (positive/negative values are interpreted as positive/negative correlations).\n",
        "print(topic_form_corr.shape) # (#topics, #forms)\n",
        "print(topic_form_corr)"
      ],
      "metadata": {
        "id": "DqZVyZT7pyIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_topic_corr = lsa_model.transform(matrix) # Contains a description of each documents in terms of the topics (positive/negative values are interpreted as positive/negative correlations).\n",
        "print(document_topic_corr.shape) # (#documents, #topics)\n",
        "print(document_topic_corr)"
      ],
      "metadata": {
        "id": "NsKkW_wOpziW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal here is to find, for each topic, the forms of max and min weight.\n",
        "\n",
        "When using the controlled dataset (and a relevant value for `lsa_n_topics`), most topics should be strongly correlated with forms all using letters close to each other in the alphabet (note that during the generation of the dataset, we have considered the alphabet to be circular, in the sense that its end and its beginning are considered to be adjacent), and/or strongly anti-correlated with forms all using letters close to each other in the alphabet."
      ],
      "metadata": {
        "id": "iRn6Zbmip1ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_forms = 10 # For each topic, the `n_forms` forms the most correlated with the topic, and the `n_forms` forms the most anti-correlated with the topic, are displayed.\n",
        "def lsa_show_topic(topic_id):\n",
        "    print(f\"Topic n°{topic_id}:\")\n",
        "\n",
        "    topic_vector = topic_form_corr[topic_id]\n",
        "    #print(topic_vector) # From id (int) to score (float).\n",
        "\n",
        "    # TODO\n",
        "    positive_corr_id = ()\n",
        "    negative_corr_id = ()\n",
        "\n",
        "    print(f\"positive correlation: {[(id2form[i], topic_vector[i]) for i in positive_corr_id]}\")\n",
        "    print(f\"negative correlation: {[(id2form[i], topic_vector[i]) for i in negative_corr_id]}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "for topic_id in range(lsa_n_topics): lsa_show_topic(topic_id) # The first topics tend to be the most important."
      ],
      "metadata": {
        "id": "LxeAIeEhp2f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latente Dirichlet Allocation"
      ],
      "metadata": {
        "id": "R78Ryblup8At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning a model"
      ],
      "metadata": {
        "id": "XzKmw1_xp9Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "if(use_controlled_dataset): lda_n_topics = n_topics\n",
        "else: lda_n_topics = 32 # Find a good value by trial and error.\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components=lda_n_topics, max_iter=20, n_jobs=-1)\n",
        "\n",
        "print(\"Fitting the model…\", end=\"\", flush=True)\n",
        "lda_model.fit(X=matrix)\n",
        "print(\" Done!\")\n",
        "\n",
        "topic_term_dists = lda_model.components_ / np.expand_dims(lda_model.components_.sum(axis=1), axis=-1) # Contains, for each topic, the probability distribution of generation over forms.\n",
        "doc_topic_dists = lda_model.transform(matrix) # Contains, for each topic, the probability distribution over topics."
      ],
      "metadata": {
        "id": "u_2Yvsjap4xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "PFUSi6Ffp__P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal here is to find, for each topic, the forms of max probability.\n",
        "\n",
        "Some topics might be highly \"spread out\", in the sense that they have a more or less uniform distribution over the vocabulary.\n",
        "These topics are not very informative and one can ignore them later.\n",
        "\n",
        "When using the controlled dataset (and a relevant value for `lda_n_topics`), most topics should generate forms all using letters close to each other in the alphabet."
      ],
      "metadata": {
        "id": "QIznJyE6qCcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_forms = 20 # For each topic, the `n_forms` most probably forms are displayed.\n",
        "def lda_show_topic(topic_id):\n",
        "    print(f\"Topic n°{topic_id}:\")\n",
        "\n",
        "    topic_vector = topic_term_dists[topic_id]\n",
        "    #print(topic_vector) # From id (int) to score (float).\n",
        "\n",
        "    # TODO\n",
        "    significant_id = ()\n",
        "\n",
        "    print([(id2form[i], topic_vector[i]) for i in significant_id])\n",
        "\n",
        "    print()\n",
        "\n",
        "for topic_id in range(lda_n_topics): lda_show_topic(topic_id)"
      ],
      "metadata": {
        "id": "zGDUkc_2qF4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each document, we look at its main topics: the one are the most strongly associated with it (i.e. that are the most strongly involved in the generation of the document according to the model).\n",
        "The set of all main topics (for all documents) is recorded in `major_topics`.\n",
        "\n",
        "When using the controlled dataset (and a relevant value for `lda_n_topics`), the first main topic of each document should almost always match the letter in the name of the document (this can be checked by looking in the block just above or alternatively in the block below)."
      ],
      "metadata": {
        "id": "-ze_06WsqKTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "major_topics = set()\n",
        "\n",
        "for (document_id, document_vector) in enumerate(doc_topic_dists):\n",
        "    #print(f\"Document n°{document_id}:\")\n",
        "    print(f\"{dataset[document_id]['str_id']}:\")\n",
        "\n",
        "    #print(document_vector) # From id (int) to score (float).\n",
        "\n",
        "    sorted_id = document_vector.argsort() # https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "    sorted_id = np.flip(sorted_id) # https://numpy.org/doc/stable/reference/generated/numpy.flip.html\n",
        "    major_id = sorted_id[:n_forms]\n",
        "    major_id = [i for i in major_id if(document_vector[i] > (0.1 * document_vector.max()))]\n",
        "    #major_id = [i for i in major_id if(document_vector[i] > (1.1 * document_vector.min()))]\n",
        "\n",
        "    print(major_id)\n",
        "    print([document_vector[i] for i in major_id]) # Prints the probability within this document of each major topic.\n",
        "    major_topics.update(major_id)\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "Xw3FNJk5qIHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(major_topics)\n",
        "print()\n",
        "\n",
        "for topic_id in major_topics: lda_show_topic(topic_id)"
      ],
      "metadata": {
        "id": "nNUOZDUgqN6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We here create a graph that shows the evolution of the topics in the documents of the dataset.\n",
        "\n",
        "When using the controlled dataset (and a relevant value for `lda_n_topics`), each topic should clearly \"peak\" on a small region of documents and quickly disappear outside of it."
      ],
      "metadata": {
        "id": "Em8sL1CkqPLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#major_topics_only = True\n",
        "major_topics_only = False\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "\n",
        "time = np.arange(len(dataset))\n",
        "for i, topic_evolution in enumerate(np.transpose(lda_model.transform(matrix))):\n",
        "    if(major_topics_only and (i not in major_topics)): continue # Filters out the topics that are not major ones.\n",
        "    ax.plot(time, topic_evolution, label=f\"topic {i}\")\n",
        "\n",
        "ax.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KIb_n8lxqQyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use here the pyLDAvis library (https://pyldavis.readthedocs.io/en/latest/readme.html) to visualise the LDA model.\n",
        "\n",
        "The interface mainly consist of a left pannel, that spatially organises the topics, and of a right pannel, that shows the vocabulary distribution of one of them.\n",
        "When a topic is selected, the right pannel shows the forms that are the most \"relevant\" to this topic.\n",
        "The notion of relevance used dependens on a parameter λ that can be manually adjusted on top of the pannel and that is described at the bottom of the pannel.\n",
        "With λ=1, the most relevant forms are the ones that are the most frequently generated by the topic.\n",
        "With λ=1, the most relevant forms are the ones that are the most specific to the topic.\n",
        "(Remember that according to the LDA model, multiple topics may generate tokens of the same form.)\n",
        "\n",
        "Note that the numbering of the topics here is not necessarily the one used above.\n",
        "This tool sorts the topics by decreasing order of importance in the dataset, the importance of a topic being the number of tokens it has generated in the corpus according to the LDA model.\n",
        "\n",
        "When using the controlled dataset (and a relevant value for `lda_n_topics`), the topics should roughly be of equal size, rarely overlap, and consists of forms almost all using the same letter."
      ],
      "metadata": {
        "id": "-mmmW0iyqSqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import pyLDAvis\n",
        "except:\n",
        "    !pip install pyLDAvis==3.3.1\n",
        "    import pyLDAvis\n",
        "\n",
        "vis_data = pyLDAvis.prepare(topic_term_dists=topic_term_dists, doc_topic_dists=doc_topic_dists, doc_lengths=doc_lengths, vocab=id2form, term_frequency=term_frequency, mds='mmds') # https://pyldavis.readthedocs.io/en/latest/modules/API.html#pyLDAvis.prepare\n",
        "pyLDAvis.display(vis_data) # Warning: This tool numbers the topics according to their importance in the corpus (in term of number of tokens). It is possible to recognise a topic by looking at the list of forms displayed for λ=1. The list displayed for λ=0 shows the forms that are the most specific to the selected topic."
      ],
      "metadata": {
        "id": "EPrRcw_NqT8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}