{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zkSo7F0RWFVv",
        "outputId": "67e6c13c-6873-41f8-88ae-e0dcb0996a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting icecream\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting colorama>=0.3.9 (from icecream)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.16.1)\n",
            "Collecting executing>=0.3.1 (from icecream)\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream\n",
            "Successfully installed asttokens-2.4.1 colorama-0.4.6 executing-2.0.1 icecream-2.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install icecream\n",
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgKHfkBwh2LA"
      },
      "source": [
        "Copy this notebook (File>Save a copy in Drive) and then work on your copy.\n",
        "==\n",
        "To send me your work: use the sharing menu (top-right of the window) to share it with timothee.m.r.bernard@gmail.com.\n",
        "(I don't check this address very often, so, for questions, please use Moodle or my u-paris.fr address.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTLyS0rZh9Ql"
      },
      "source": [
        "Goal\n",
        "==\n",
        "We are about to train a *sequence-to-sequence model* to predict a paragraph of Gustave Flaubert's *Madame Bovary* given the preceding paragraph.\n",
        "The model (at least in its first version) does not use words as units of text but characters.\n",
        "\n",
        "*   The encoder part, based on a bidirectional LSTM, reads an input paragraph and turns it into a set of tensors that serves as initial state for the decoder part.\n",
        "*   The decoder part is based on an (unidirectional) LSTM. The state of the LSTM is used to compute a probability distribution over the alphabet (including space and punctuation marks) and is updated each time a character is predicted by the LSTM reading this character's embedding.\n",
        "*   The goal is to get the best model. It is part of the job to define what this means. It is also part of the job to explain me how you get your best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytOCVOAXoO2s"
      },
      "source": [
        "This is an assignment.\n",
        "==\n",
        "\n",
        "*   Work in groups of two or three students.\n",
        "*   Due date: December 4th (Monday), 23:59\n",
        "*   Malus: -1 per day of delay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZWD-p9yiECs"
      },
      "source": [
        "Loading PyTorch is important.\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "An16FNHuhZI1"
      },
      "outputs": [],
      "source": [
        "# Imports PyTorch.\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8IdgwKsx1P2"
      },
      "source": [
        "Remarks:\n",
        "==\n",
        "*   Follow the instructions very carefully. Do not ignore any comment.\n",
        "*   Keep in mind all remarks given in previous TPs.\n",
        "*   Comment your code (including the role of all functions and the type of their arguments). A piece of code not appropriately commented can be considered incorrect (irrespectively of whether it works or not).\n",
        "*   Indicate the shape of each tensor that you define.\n",
        "*   Document all the changes that you make. Any work that is not properly explained can be ignored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2oBAYuiZPX"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use is there: \"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\"\n",
        "\n",
        "We have to pre-process it a little bit in order to remove everything that is not part of the text and to split the actual text into paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "veOPiPCIlOOj"
      },
      "outputs": [],
      "source": [
        "use_toy_dataset = False # If True, a toy dataset (see below) is use instead of the real one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0VXDpa1tiSfQ",
        "outputId": "1eb835af-b679-419a-8fcb-b29e3a6ac1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/tmp42x1hwae\n"
          ]
        }
      ],
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\")\n",
        "filename = tmp[0]\n",
        "print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bU4hirpsiWX2",
        "outputId": "8d6fc9cd-af79-4337-bc07-93c8a05ee548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0] ﻿The Project Gutenberg eBook of Madame Bovary\n",
            "[1]     \n",
            "[2] This ebook is for the use of anyone anywhere in the United States and\n",
            "[3] most other parts of the world at no cost and with almost no restrictions\n",
            "[4] whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "[5] of the Project Gutenberg License included with this ebook or online\n",
            "[6] at www.gutenberg.org. If you are not located in the United States,\n",
            "[7] you will have to check the laws of the country where you are located\n",
            "[8] before using this eBook.\n",
            "[9] \n",
            "[10] Title: Madame Bovary\n",
            "[11] \n",
            "[12] \n",
            "[13] Author: Gustave Flaubert\n",
            "[14] \n",
            "[15] Release date: November 26, 2004 [eBook #14155]\n",
            "[16]                 Most recently updated: December 18, 2020\n",
            "[17] \n",
            "[18] Language: French\n",
            "[19] \n",
            "[20] \n",
            "[21] \n",
            "[22] *** START OF THE PROJECT GUTENBERG EBOOK MADAME BOVARY ***\n",
            "[23] \n",
            "[24] \n",
            "[25] \n",
            "[26] Produced by Ebooks libres et gratuits at http://www.ebooksgratuits.com\n",
            "[27] \n",
            "[28] \n",
            "[29] \n",
            "[30] \n",
            "[31] \n",
            "[32] Gustave Flaubert\n",
            "[33] MADAME BOVARY\n",
            "[34] \n",
            "[35] \n",
            "[36] (1857)\n",
            "[37] \n",
            "[38] \n",
            "[39] Table des matières\n",
            "[40] \n",
            "[41] PREMIÈRE PARTIE\n",
            "[42] I\n",
            "[43] II\n",
            "[44] III\n",
            "[45] IV\n",
            "[46] V\n",
            "[47] VI\n",
            "[48] VII\n",
            "[49] VIII\n",
            "[50] IX\n",
            "[51] DEUXIÈME PARTIE\n",
            "[52] I\n",
            "[53] II\n",
            "[54] III\n",
            "[55] IV\n",
            "[56] V\n",
            "[57] VI\n",
            "[58] VII\n",
            "[59] VIII\n",
            "[60] IX\n",
            "[61] X\n",
            "[62] XI\n",
            "[63] XII\n",
            "[64] XIII\n",
            "[65] XIV\n",
            "[66] XV\n",
            "[67] TROISIÈME PARTIE\n",
            "[68] I\n",
            "[69] II\n",
            "[70] III\n",
            "[71] IV\n",
            "[72] V\n",
            "[73] VI\n",
            "[74] VII\n",
            "[75] VIII\n",
            "[76] IX\n",
            "[77] X\n",
            "[78] XI\n",
            "[79] \n",
            "[80] \n",
            "[81] À Marie-Antoine-Jules Senard\n",
            "[82] \n",
            "[83] MEMBRE DU BARREAU DE PARIS EX-PRESIDENT DE L'ASSEMBLÉE NATIONALE\n",
            "[84] ET ANCIEN MINISTRE DE L'INTÉRIEUR\n",
            "[85] \n",
            "[86] Cher et illustre ami,\n",
            "[87] \n",
            "[88] Permettez-moi d'inscrire votre nom en tête de ce livre et au-\n",
            "[89] dessus même de sa dédicace; car c'est à vous, surtout, que j'en\n",
            "[90] dois la publication. En passant par votre magnifique plaidoirie,\n",
            "[91] mon oeuvre a acquis pour moi-même comme une autorité imprévue.\n",
            "[92] Acceptez donc ici l'hommage de ma gratitude, qui, si grande\n",
            "[93] qu'elle puisse être, ne sera jamais à la hauteur de votre\n",
            "[94] éloquence et de votre dévouement.\n",
            "[95] \n",
            "[96] GUSTAVE FLAUBERT\n",
            "[97] \n",
            "[98] Paris, 12 avril 1857\n",
            "[99] \n",
            "[100] \n",
            "[101] À Louis Bouilhet\n",
            "[102] \n",
            "[103] \n",
            "[104] PREMIÈRE PARTIE\n",
            "[105] \n",
            "[106] \n",
            "[107] I\n",
            "[108] \n",
            "[109] Nous étions à l'Étude, quand le Proviseur entra, suivi d'un\n",
            "[110] nouveau habillé en bourgeois et d'un garçon de classe qui portait\n",
            "[111] un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se\n",
            "[112] leva comme surpris dans son travail.\n",
            "[113] \n",
            "[114] Le Proviseur nous fit signe de nous rasseoir; puis, se tournant\n",
            "[115] vers le maître d'études:\n",
            "[116] \n",
            "[117] -- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je\n",
            "[118] vous recommande, il entre en cinquième. Si son travail et sa\n",
            "[119] conduite sont méritoires, il passera dans les grands, où l'appelle\n",
            "[120] son âge.\n",
            "[121] \n",
            "[122] Resté dans l'angle, derrière la porte, si bien qu'on l'apercevait\n",
            "[123] à peine, le nouveau était un gars de la campagne, d'une quinzaine\n",
            "[124] d'années environ, et plus haut de taille qu'aucun de nous tous. Il\n",
            "[125] avait les cheveux coupés droit sur le front, comme un chantre de\n",
            "[126] village, l'air raisonnable et fort embarrassé. Quoiqu'il ne fût\n",
            "[127] pas large des épaules, son habit-veste de drap vert à boutons\n",
            "[128] noirs devait le gêner aux entournures et laissait voir, par la\n",
            "[129] fente des parements, des poignets rouges habitués à être nus. Ses\n",
            "[130] jambes, en bas bleus, sortaient d'un pantalon jaunâtre très tiré\n",
            "[131] par les bretelles. Il était chaussé de souliers forts, mal cirés,\n",
            "[132] garnis de clous.\n",
            "[133] \n",
            "[134] On commença la récitation des leçons. Il les écouta de toutes ses\n",
            "[135] oreilles, attentif comme au sermon, n'osant même croiser les\n",
            "[136] cuisses, ni s'appuyer sur le coude, et, à deux heures, quand la\n",
            "[137] cloche sonna, le maître d'études fut obligé de l'avertir, pour\n",
            "[138] qu'il se mît avec nous dans les rangs.\n",
            "[139] \n",
            "[140] Nous avions l'habitude, en entrant en classe, de jeter nos\n",
            "[141] casquettes par terre, afin d'avoir ensuite nos mains plus libres;\n",
            "[142] il fallait, dès le seuil de la porte, les lancer sous le banc, de\n",
            "[143] façon à frapper contre la muraille en faisant beaucoup de\n",
            "[144] poussière; c'était là le genre.\n",
            "[145] \n",
            "[146] Mais, soit qu'il n'eût pas remarqué cette manoeuvre ou qu'il n'eut\n",
            "[147] osé s'y soumettre, la prière était finie que le nouveau tenait\n",
            "[148] encore sa casquette sur ses deux genoux. C'était une de ces\n",
            "[149] coiffures d'ordre composite, où l'on retrouve les éléments du\n",
            "[150] bonnet à poil, du chapska, du chapeau rond, de la casquette de\n",
            "[151] loutre et du bonnet de coton, une de ces pauvres choses, enfin,\n",
            "[152] dont la laideur muette a des profondeurs d'expression comme le\n",
            "[153] visage d'un imbécile. Ovoïde et renflée de baleines, elle\n",
            "[154] commençait par trois boudins circulaires; puis s'alternaient,\n",
            "[155] séparés par une bande rouge, des losanges de velours et de poils\n",
            "[156] de lapin; venait ensuite une façon de sac qui se terminait par un\n",
            "[157] polygone cartonné, couvert d'une broderie en soutache compliquée,\n",
            "[158] et d'où pendait, au bout d'un long cordon trop mince, un petit\n",
            "[159] croisillon de fils d'or, en manière de gland. Elle était neuve; la\n",
            "[160] visière brillait.\n",
            "[161] \n",
            "[162] -- Levez-vous, dit le professeur.\n",
            "[163] \n",
            "[164] Il se leva; sa casquette tomba. Toute la classe se mit à rire.\n",
            "[165] \n",
            "[166] Il se baissa pour la reprendre. Un voisin la fit tomber d'un coup\n",
            "[167] de coude, il la ramassa encore une fois.\n",
            "[168] \n",
            "[169] -- Débarrassez-vous donc de votre casque, dit le professeur, qui\n",
            "[170] était un homme d'esprit.\n",
            "[171] \n",
            "[172] Il y eut un rire éclatant des écoliers qui décontenança le pauvre\n",
            "[173] garçon, si bien qu'il ne savait s'il fallait garder sa casquette à\n",
            "[174] la main, la laisser par terre ou la mettre sur sa tête. Il se\n",
            "[175] rassit et la posa sur ses genoux.\n",
            "[176] \n",
            "[177] -- Levez-vous, reprit le professeur, et dites-moi votre nom.\n",
            "[178] \n",
            "[179] Le nouveau articula, d'une voix bredouillante, un nom\n",
            "[180] inintelligible.\n",
            "[181] \n",
            "[182] -- Répétez!\n",
            "[183] \n",
            "[184] Le même bredouillement de syllabes se fit entendre, couvert par\n",
            "[185] les huées de la classe.\n",
            "[186] \n",
            "[187] -- Plus haut! cria le maître, plus haut!\n",
            "[188] \n",
            "[189] Le nouveau, prenant alors une résolution extrême, ouvrit une\n",
            "[190] bouche démesurée et lança à pleins poumons, comme pour appeler\n",
            "[191] quelqu'un, ce mot: Charbovari.\n",
            "[192] \n",
            "[193] Ce fut un vacarme qui s'élança d'un bond, monta en crescendo, avec\n",
            "[194] des éclats de voix aigus (on hurlait, on aboyait, on trépignait,\n",
            "[195] on répétait: Charbovari! Charbovari!), puis qui roula en notes\n",
            "[196] isolées, se calmant à grand-peine, et parfois qui reprenait tout à\n",
            "[197] coup sur la ligne d'un banc où saillissait encore çà et là, comme\n",
            "[198] un pétard mal éteint, quelque rire étouffé.\n",
            "[199] \n"
          ]
        }
      ],
      "source": [
        "# Prints the first 200 lines in the file with their line number.\n",
        "# This shows that we have a little bit of preprocessing to do in order to clean the data.\n",
        "with open(filename) as f:\n",
        "  for i in range(200):\n",
        "    print(f\"[{i}] {f.readline()}\", end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ctl4Z9Gti-6U",
        "outputId": "7f018bb4-5eb2-4087-ba0c-5e4df3e2a3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2995 paragraphs read.\n",
            "Nous étions à l'Étude, quand le Proviseur entra, suivi d'un nouveau habillé en bourgeois et d'un garçon de classe qui portait un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se leva comme surpris dans son travail.\n",
            "Le Proviseur nous fit signe de nous rasseoir; puis, se tournant vers le maître d'études:\n",
            "-- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je vous recommande, il entre en cinquième. Si son travail et sa conduite sont méritoires, il passera dans les grands, où l'appelle son âge.\n"
          ]
        }
      ],
      "source": [
        "import re # Regular expression library\n",
        "roman_regex = re.compile('^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$') # This regular expression matches Roman numerals but also the empty string.\n",
        "\n",
        "EOP = '\\n' # The end-of-line character will be used to mark the end of paragraphs.\n",
        "\n",
        "with open(filename) as f:\n",
        "  # We want to skip everything before the actual text of the novel.\n",
        "  # The line \"PREMIÈRE PARTIE\" appears twice: in the table of content and then at the start of the first part of the actual text.\n",
        "  # The following lines discard everything up to this second occurence (included).\n",
        "  skip = 2\n",
        "  while(skip > 0):\n",
        "    line = f.readline().strip()\n",
        "    if(line == \"PREMIÈRE PARTIE\"): skip -= 1;\n",
        "\n",
        "  paragraphs = [] # Note that each dialog line will be considered a separate paragraph.\n",
        "  paragraph_buffer = [] # List[str]; each element corresponds to a line in the original text file + an additonal space if necessary.\n",
        "  while(True):\n",
        "    line = f.readline().strip()\n",
        "    if(\"END OF THE PROJECT GUTENBERG EBOOK MADAME BOVARY\" in line): break # End of the actual text.\n",
        "\n",
        "    if(line == \"\"): # We've reached the end of a paragraph.\n",
        "      if(len(paragraph_buffer) > 0):\n",
        "        paragraph_buffer.append(EOP) # End of the paragraph.\n",
        "\n",
        "        paragraph = \"\".join(paragraph_buffer) # The different lines that make up the paragraph are joined into a single string.\n",
        "        paragraphs.append(paragraph)\n",
        "        paragraph_buffer = []\n",
        "      continue\n",
        "\n",
        "    if(roman_regex.match(line)): continue # Ignores the lines that indicate the beginning of a chapter.\n",
        "    if(line.endswith(\" PARTIE\")): continue # Ignores the lines that indicate the beginning of a part.\n",
        "\n",
        "    if((len(paragraph_buffer) > 0) and (paragraph_buffer[-1][-1] != '-')): paragraph_buffer.append(' ') # Adds a space between consecutive lines except when the first one ends with \"-\" (e.g. if the word \"pomme-de-terre\" is split with \"pomme-de-\" at the end of a line and \"terre\" at the beginning of the next, we do not want to join the two lines with a space).\n",
        "    paragraph_buffer.append(line)\n",
        "\n",
        "print(f\"{len(paragraphs)} paragraphs read.\")\n",
        "for i in range(3): print(paragraphs[i], end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZttAP5uj6y2"
      },
      "source": [
        "Here, we define a toy dataset on which your model, if correctly implemented, should be able to learn more easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "28V5KvT6jUFA"
      },
      "outputs": [],
      "source": [
        "if(use_toy_dataset):\n",
        "  paragraphs = []\n",
        "\n",
        "  import random, string\n",
        "  characters = list(string.ascii_lowercase + string.ascii_lowercase.upper() + \"_-/\\'[]()\")\n",
        "  random.shuffle(characters)\n",
        "  k = random.randint(1, 10)\n",
        "  a = \"a\"\n",
        "  paragraph = (a * k)\n",
        "  for _ in range(100):\n",
        "    random.shuffle(characters)\n",
        "    for a in characters:\n",
        "      k = random.randint(1, 16)\n",
        "      paragraph += f\"? Now, please write {k} {a}.{EOP}\"\n",
        "      paragraphs.append(paragraph)\n",
        "      paragraph = (a * k)\n",
        "  print(f\"{len(paragraphs)} paragraphs generated.\")\n",
        "\n",
        "  print(paragraphs[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PAn1SqQE1Hr-",
        "outputId": "0f20b50d-707c-473f-84e5-33fe4fcef9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94 different characters found in the dataset.\n",
            "[(' ', 109336), ('e', 76369), ('a', 44997), ('s', 42972), ('t', 38893), ('i', 38322), ('n', 35099), ('r', 34314), ('l', 33711), ('u', 32914), ('o', 27415), ('d', 19187), ('c', 14786), ('m', 14638), ('p', 13790), (',', 12378), ('v', 8441), ('é', 8263), (\"'\", 7451), ('.', 6225), ('b', 5519), ('q', 5455), ('f', 5406), ('h', 5386), ('g', 4704), ('-', 4243), ('\\n', 2995), ('à', 2722), ('x', 2057), ('j', 1728), ('è', 1644), ('y', 1619), ('!', 1512), ('E', 1477), (';', 1425), ('ê', 1188), ('L', 981), ('C', 945), ('I', 769), ('M', 743), ('z', 674), ('A', 543), ('?', 530), (':', 480), ('ç', 470), ('B', 427), ('â', 410), ('P', 394), ('î', 327), ('R', 319), ('D', 313), ('O', 301), ('S', 298), ('ô', 296), ('ù', 293), ('H', 270), ('û', 241), ('Q', 237), ('J', 233), ('T', 211), ('V', 181), ('N', 155), ('U', 122), ('«', 120), ('»', 112), ('À', 84), ('F', 84), ('Y', 80), ('_', 64), ('G', 62), ('(', 55), (')', 55), ('ï', 37), ('É', 25), ('k', 16), ('1', 16), ('ë', 9), ('2', 8), ('9', 8), ('3', 8), ('Ç', 7), ('X', 6), ('6', 6), ('5', 5), ('4', 5), ('ü', 5), ('8', 4), ('Ê', 4), ('°', 4), ('W', 3), ('7', 3), ('w', 2), ('0', 1), ('Î', 1)]\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# Computes the frequency of all characters in the dataset.\n",
        "char_counts = collections.defaultdict(int)\n",
        "for paragraph in paragraphs:\n",
        "  for char in paragraph: char_counts[char] += 1\n",
        "\n",
        "print(f\"{len(char_counts)} different characters found in the dataset.\")\n",
        "print(sorted(char_counts.items(), key=(lambda x: x[1]), reverse=True)) # Shows each character with its frequency, in decreasing frequency order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nF0YFZTxD_A0"
      },
      "outputs": [],
      "source": [
        "# Here you have to build a dictionary 'char_vocabulary' that assigns an integer id to each character, along with a list/array 'id_to_char' that implements the reverse mapping.\n",
        "#################\n",
        "id_to_char = [char for char in char_counts.keys()]\n",
        "char_vocabulary = {char:i for i, char in enumerate(id_to_char)}\n",
        "#################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C71VlK5e3Gg4",
        "outputId": "95bb77ea-aa4c-420b-d3fd-133140f1a685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'N': 0, 'o': 1, 'u': 2, 's': 3, ' ': 4, 'é': 5, 't': 6, 'i': 7, 'n': 8, 'à': 9, 'l': 10, \"'\": 11, 'É': 12, 'd': 13, 'e': 14, ',': 15, 'q': 16, 'a': 17, 'P': 18, 'r': 19, 'v': 20, 'h': 21, 'b': 22, 'g': 23, 'ç': 24, 'c': 25, 'p': 26, '.': 27, 'C': 28, 'x': 29, 'm': 30, 'è': 31, '\\n': 32, 'L': 33, 'f': 34, ';': 35, 'î': 36, ':': 37, '-': 38, 'M': 39, 'R': 40, 'j': 41, 'S': 42, 'ù': 43, 'â': 44, 'z': 45, 'I': 46, 'Q': 47, 'û': 48, 'ê': 49, 'O': 50, 'y': 51, 'k': 52, 'ï': 53, 'E': 54, 'T': 55, 'U': 56, 'D': 57, '!': 58, '(': 59, ')': 60, 'B': 61, '?': 62, '_': 63, 'G': 64, '1': 65, '8': 66, '2': 67, 'A': 68, 'À': 69, 'H': 70, 'ô': 71, 'V': 72, 'Ê': 73, '«': 74, '»': 75, 'Y': 76, 'F': 77, 'J': 78, 'ë': 79, 'W': 80, 'X': 81, '0': 82, '5': 83, '7': 84, '9': 85, '6': 86, '3': 87, 'w': 88, 'Î': 89, 'Ç': 90, '4': 91, 'ü': 92, '°': 93}\n",
            "['N', 'o', 'u', 's', ' ', 'é', 't', 'i', 'n', 'à', 'l', \"'\", 'É', 'd', 'e', ',', 'q', 'a', 'P', 'r', 'v', 'h', 'b', 'g', 'ç', 'c', 'p', '.', 'C', 'x', 'm', 'è', '\\n', 'L', 'f', ';', 'î', ':', '-', 'M', 'R', 'j', 'S', 'ù', 'â', 'z', 'I', 'Q', 'û', 'ê', 'O', 'y', 'k', 'ï', 'E', 'T', 'U', 'D', '!', '(', ')', 'B', '?', '_', 'G', '1', '8', '2', 'A', 'À', 'H', 'ô', 'V', 'Ê', '«', '»', 'Y', 'F', 'J', 'ë', 'W', 'X', '0', '5', '7', '9', '6', '3', 'w', 'Î', 'Ç', '4', 'ü', '°']\n",
            "EOP_id = 32\n",
            "Both mappings have the same number of elements (which is 94)\n",
            "\n",
            "All elements from char_vocabulary are in id_to_char\n",
            "All elements from id_to_char are in char_vocabulary\n"
          ]
        }
      ],
      "source": [
        "EOP_id = char_vocabulary[EOP] # Id for the end-of-paragraph symbol\n",
        "\n",
        "print(char_vocabulary)\n",
        "print(id_to_char)\n",
        "print(f\"EOP_id = {EOP_id}\")\n",
        "\n",
        "# Here you have to implement a test that proves that your implementations of 'char_vocabulary' and 'id_to_char' are consistent.\n",
        "#################\n",
        "if len(char_vocabulary) == len(id_to_char):\n",
        "  print(f\"Both mappings have the same number of elements (which is {len(id_to_char)})\")\n",
        "  print()\n",
        "for elt in char_vocabulary:\n",
        "  if elt not in id_to_char:\n",
        "    print(f\"{elt} is missing from id_to_char\")\n",
        "  else:\n",
        "    char_in_id = True\n",
        "if char_in_id: print(\"All elements from char_vocabulary are in id_to_char\")\n",
        "for elt in id_to_char:\n",
        "  if elt not in char_vocabulary:\n",
        "    print(f\"{elt} is missing from char_vocabulary\")\n",
        "  else:\n",
        "    id_in_char = True\n",
        "if id_in_char: print(\"All elements from id_to_char are in char_vocabulary\")\n",
        "#################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyD_paQGfFiO"
      },
      "source": [
        "modified below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8bc3Gst0zhQv",
        "outputId": "3db42c57-3a07-46a6-a486-1b67e64611a4"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ef8f37d607c2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Turns a list of lists of ids into a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Do not forget that an occurrence of EOP means that the paragraph ends here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mids_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Here you have to turn each list of character ids of 'ids' into a string and then return all strings as a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'type' object is not iterable"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "# Turns a list of lists of ids into a list of strings.\n",
        "# Do not forget that an occurrence of EOP means that the paragraph ends here.\n",
        "def ids_to_texts(ids:list((list(int)))) -> list(str):\n",
        "  # Here you have to turn each list of character ids of 'ids' into a string and then return all strings as a list.\n",
        "  #################\n",
        "  sentences = []\n",
        "  for char_id_list in ids:  #iter over all id sequences\n",
        "    words = \"\"\n",
        "    for id in char_id_list: #iter over all id\n",
        "\n",
        "      #type checks\n",
        "      if type(char_id_list) == list: key = id\n",
        "      if type(char_id_list) == torch.Tensor: key = id.item()\n",
        "      # If id is different from EOP'id, add the corresponding character to the string\n",
        "      if id != EOP_id:\n",
        "        words += id_to_char[key] #decode id\n",
        "      else:\n",
        "        break\n",
        "    sentences.append(words)\n",
        "  return sentences\n",
        "  #################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ybAhzb4_3RTk"
      },
      "outputs": [],
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZnT7T-yHEPCh"
      },
      "outputs": [],
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "ids[0].extend([EOP_id, (EOP_id+1), (EOP_id+1)]) # With the end-of-paragraph token id and additional (padding-like) stuff for the first string.\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\") # If you have a problem here, remember that EOP indicates the end of the text (this might be related to your problem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuCiGqO08nd"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aoH4g-Fkkrgc"
      },
      "outputs": [],
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "# A training instance is composed of a pair of consecutive paragraphs. The goal will be to predict the second given the first.\n",
        "# TODO: (Possible improvement: As is, ends of chapter are completely ignored: the last paragraph of a chapter and the first of the following chapter form a training instance. We might want to predict the end of the chapter instead, or simply remove these pairs from the dataset.)\n",
        "class BatchGenerator:\n",
        "  def __init__(self, paragraphs, char_vocabulary):\n",
        "    tr = int(len(paragraphs)*0.9)\n",
        "    self.train_paragraphs = paragraphs[:tr]\n",
        "    self.dev_paragraphs = paragraphs[tr:]\n",
        "\n",
        "    print(f\" ** train ({len(self.train_paragraphs)}) & dev ({len(self.dev_paragraphs)}) split generated ** \")\n",
        "\n",
        "    self.char_vocabulary = char_vocabulary # Dictionary\n",
        "    self.padding_idx = len(char_vocabulary)\n",
        "\n",
        "  # Returns the number of training instances (i.e. of pairs of consecutive paragraphs).\n",
        "  def length(self, split):\n",
        "    if split == 'train':\n",
        "      return (len(self.train_paragraphs) - 1)\n",
        "    if split == 'dev':\n",
        "      return (len(self.dev_paragraphs) - 1)\n",
        "\n",
        "  # Returns a random training batch (composed of pairs of consecutive paragraphs).\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful when debugging the system.\n",
        "  def get_batch(self, batch_size, split, subset=None):\n",
        "    max_i = self.length(split) if(subset is None) else min(subset, self.length(split))\n",
        "    paragraph_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some paragraph ids.\n",
        "\n",
        "    return self._ids_to_batch(paragraph_ids, split)\n",
        "\n",
        "  def _ids_to_batch(self, paragraph_ids, split):\n",
        "    firsts = [] # First paragraph of each pair\n",
        "    seconds = [] # Second paragraph of each pair\n",
        "    for paragraph_id in paragraph_ids:\n",
        "      if split == 'train':\n",
        "        firsts.append([self.char_vocabulary[char] for char in self.train_paragraphs[paragraph_id]])\n",
        "        seconds.append([self.char_vocabulary[char] for char in self.train_paragraphs[paragraph_id + 1]])\n",
        "      if split == 'dev':\n",
        "        firsts.append([self.char_vocabulary[char] for char in self.dev_paragraphs[paragraph_id]])\n",
        "        seconds.append([self.char_vocabulary[char] for char in self.dev_paragraphs[paragraph_id + 1]])\n",
        "\n",
        "    # Padding\n",
        "    self.pad(firsts)\n",
        "    self.pad(seconds)\n",
        "\n",
        "    firsts = torch.tensor(firsts, dtype=torch.long) # Conversion to a tensor\n",
        "    seconds = torch.tensor(seconds, dtype=torch.long) # Conversion to a tensor\n",
        "\n",
        "    return (firsts, seconds)\n",
        "\n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, sequences):\n",
        "    max_length = max([len(s) for s in sequences])\n",
        "    for s in sequences: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "\n",
        "  # Returns a generator of training batches for a full epoch. (Note that this function is not used in the training loop implemented below. `get_batch` is used instead.)\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful when debugging the system.\n",
        "  def all_batches(self, batch_size, split, subset=None):\n",
        "    max_i = self.length(split) if(subset is None) else min(subset, self.length(split))\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size').\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(instance_ids, split)\n",
        "      i += batch_size\n",
        "\n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(instance_ids, split)\n",
        "\n",
        "  # Turns a list of arbitrary paragraphs into a prediction batch.\n",
        "  def turn_into_batch(self, paragraphs):\n",
        "    firsts = []\n",
        "    for paragraph in paragraphs:\n",
        "        # Unknown characters are ignored (removed).\n",
        "        tmp = []\n",
        "        for char in paragraph:\n",
        "          if(char in self.char_vocabulary): tmp.append(self.char_vocabulary[char])\n",
        "\n",
        "        if(tmp[-1] != EOP_id): tmp.append(EOP_id) # Adds an end-of-paragraph character if necessary.\n",
        "\n",
        "        firsts.append(tmp)\n",
        "\n",
        "    self.pad(firsts)\n",
        "    return torch.tensor(firsts, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(paragraphs=paragraphs, char_vocabulary=char_vocabulary)\n",
        "print(batch_generator.length('train'))\n",
        "print(batch_generator.length('dev'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_YSugGI7z3JX"
      },
      "outputs": [],
      "source": [
        "(firsts, seconds) = batch_generator.get_batch(3, 'dev')\n",
        "print(ids_to_texts(firsts))\n",
        "print(ids_to_texts(seconds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1Z1s-cQWiT"
      },
      "source": [
        "The model\n",
        "==\n",
        "For this model, we will not define a `forward` method, but two methods: `trainingLogits` and `predictionStrings`.\n",
        "\n",
        "*    `trainingLogits` is used at training time, when each batch is split in two parts: input paragraphs and output paragraphs. This function outputs, for each output paragraph of the batch, a log-probability distribution (i.e. a vector of \"logits\") before each token and after the last one. These distributions depend on the encoding of the corresponding input paragraph. They will then be used to compute a loss value.\n",
        "*    `predictionStrings` is used at prediction time, when each batch is only composed of input paragraphs. This function outputs, for each input paragraph, a string obtained by decoding the encoding of the paragraph.\n",
        "\n",
        "(Don't forget to read carefully all comments and to make sure that you understand them.)\n",
        "\n",
        "Here is a graphical representation of the architecture: https://moodle.u-paris.fr/mod/resource/view.php?id=648001\n",
        "Before starting the implementation, make sure you understand it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MppA-JDx4Se"
      },
      "source": [
        "modified below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tKfRCXQOOm8X"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "  # 'size_vocabulary' does not include a padding character, but does include the end-of-paragraph one.\n",
        "  def __init__(self, size_vocabulary, EOP_id, embedding_dim, lstm_hidden_size, lstm_layers, device='cpu'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.EOP_id = EOP_id # At prediction time, this index is used to stop the generation at the end of the paragraph.\n",
        "\n",
        "    # Here you have to define:\n",
        "    #################\n",
        "    #  (i) an embedding layer 'self.char_embeddings' with 'torch.nn.Embedding' for the characters, including an padding embedding;\n",
        "    self.char_embeddings = torch.nn.Embedding(num_embeddings = size_vocabulary+1, # need to add an additional embedding for padding\n",
        "                                              embedding_dim  = embedding_dim,\n",
        "                                              padding_idx=size_vocabulary) # padding token is at end of vocabulary\n",
        "\n",
        "    #  (ii) a bidirectional LSTM 'self.encoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    self.encoder_lstm = torch.nn.LSTM(input_size    = embedding_dim,        # encoder's inputs are token embeddings\n",
        "                                      hidden_size   = lstm_hidden_size,     # size of hidden size of lstm, an hyperparameter\n",
        "                                      num_layers    = lstm_layers,          # stacks num_layers lstms on each others\n",
        "                                      batch_first   = True,                 # we want batches to be on first layer\n",
        "                                      bidirectional = True)                 # encoder is bidirectional since at encoding time we have access to the entire sentence\n",
        "                                                                            # LSTM OUTPUT = output        (shape: batch_size, sequence_length, 2 * hidden_size),\n",
        "                                                                            #               hidden_state  (shape: batch_size, lstm_layers, lstm_hidden_size)\n",
        "                                                                            #               cell_state    (shape: batch_size, lstm_layers, lstm_hidden_size)\n",
        "\n",
        "    #  (iii) a unidirectional LSTM 'self.decoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    self.decoder_initialiser = torch.nn.Sequential(torch.nn.Linear(in_features= 2* lstm_hidden_size,    # output from encoder (is bidirectional)\n",
        "                                                                   out_features= lstm_hidden_size),     # input for decoder   (is unidirectional)\n",
        "                                                   torch.nn.ELU())\n",
        "\n",
        "    #  (iv) a network 'self.decoder_initialiser' meant to turn the final hidden and cell states of the encoder into the initial hidden and cell states of the decoder;\n",
        "    self.decoder_lstm = torch.nn.LSTM(input_size    = embedding_dim,        # decoder's input are also token embeddings\n",
        "                                      hidden_size   = lstm_hidden_size,     # size of hidden size of lstm, an hyperparameter\n",
        "                                      num_layers    = lstm_layers,          # stacks num_layers lstms on each others\n",
        "                                      batch_first   = True,                 # we want batches to be on first layer\n",
        "                                      bidirectional = False)                # decoder is unidirectional since at decoding time we only have information about left context\n",
        "                                                                            # LSTM OUTPUT = output        (shape: batch_size, sequence_length, 2 * hidden_size),\n",
        "                                                                            #               hidden_state  (shape: batch_size, lstm_layers, lstm_hidden_size)\n",
        "                                                                            #               cell_state    (shape: batch_size, lstm_layers, lstm_hidden_size)\n",
        "\n",
        "    #  (v) a network 'self.distribution_nn' meant to turn the hidden state of the decoder at each step into the logits of a probability distribution over the vocabulary. The logits of a probability distribution are simply the log-probabilities (you might want to use torch.nn.LogSoftmax).\n",
        "    self.distribution_nn = torch.nn.Sequential(torch.nn.Linear(in_features  = lstm_hidden_size,   # output from decoder serves as input to final head\n",
        "                                                               out_features = size_vocabulary),   # final output is a probability distribution over vocabulary\n",
        "                                               torch.nn.LogSoftmax(dim=-1))\n",
        "\n",
        "    # Send all parts to 'device', so that we can use a GPU.\n",
        "    self.char_embeddings.to(device)\n",
        "    self.encoder_lstm.to(device)\n",
        "    self.decoder_initialiser.to(device)\n",
        "    self.decoder_lstm.to(device)\n",
        "    self.distribution_nn.to(device)\n",
        "    #################\n",
        "\n",
        "  # This function encodes the input paragraphs and turns them into initial states for the decoder. It is used both at training and prediction time.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def initStates(self, in_paragraphs):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    in_char_embeddings = self.char_embeddings(in_paragraphs) # Shape: (batch_size, max length, embedding size)\n",
        "    #print(in_char_embeddings); print(in_char_embeddings.shape)\n",
        "    in_lengths = (in_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "    #print(in_lengths); print(in_lengths.shape)\n",
        "    in_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=in_char_embeddings, lengths=in_lengths.cpu(), batch_first=True, enforce_sorted=False) # Enables the biLSTM to ignore padding elements.\n",
        "\n",
        "    # The input paragraphs are encoded; the final hidden and cell states of the network will be used to initialise the decoder after a little transformation.\n",
        "    _, (h_n, c_n) = self.encoder_lstm(in_char_embeddings) # 'h_n' and 'c_n' are both of shape (num_layers * 2, batch_size, hidden_size)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final hidden states of the biLSTM.\n",
        "    h_n = h_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(h_n); print(h_n.shape)\n",
        "    lr_h_n = h_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_h_n = h_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_h_n = torch.cat([lr_h_n, rl_h_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_h_n); print(bi_h_n.shape)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final cell states of the biLSTM.\n",
        "    c_n = c_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(c_n); print(c_n.shape)\n",
        "    lr_c_n = c_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_c_n = c_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_c_n = torch.cat([lr_c_n, rl_c_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_c_n); print(bi_c_n.shape)\n",
        "\n",
        "    # What should be the shape of the two tensors of the following pair? Answer: for both, the output of the decoder_initializer has shape = batch_size, num_layers, lstm_hidden_size\n",
        "    return (self.decoder_initialiser(bi_h_n), self.decoder_initialiser(bi_c_n))\n",
        "\n",
        "  # Training time: This function outputs the logits for each time step.\n",
        "  # Because at training time, the output paragraph is known, there is no need to generate anything sequentially — all positions can be processed at the same time. In fact, there is a loop hidden in the call to the decoder LSTM, but you should not write any explicit loop here.\n",
        "  # Do not forget the distribution for the first character.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # 'out_paragraphs' is a matrix (batch size, max out length) of character ids (Integer) at training time. Assume it does not include the final end-of-paragraph character.\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def trainingLogits(self, in_paragraphs, out_paragraphs):\n",
        "    decoder_init_states = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "\n",
        "    # Feed a packed sequence to the decoder (use 'torch.nn.utils.rnn.pack_padded_sequence' and 'torch.nn.utils.rnn.pad_packed_sequence').\n",
        "    # You don't need to implement a loop, because at training time, you know in advance the decisions of the system (i.e. the tokens that are generated).\n",
        "    #################\n",
        "    batch_size = out_paragraphs.size(0)\n",
        "    max_out_length = out_paragraphs.size(1)\n",
        "\n",
        "    # Extract probability distribution from first hidden state. This should ideally have as argmax the index of the token at position 0\n",
        "    first_logits = self.distribution_nn(decoder_init_states[0][-1]).view(batch_size,1,-1) # Shape: (batch_size, 1, vocabulary_size)\n",
        "\n",
        "    #ignore last token, since its final probability distribution will not point toward an expected token in our sequence\n",
        "    out_paragraphs = out_paragraphs[:,range(max_out_length-1)] # Shape: (batch_size, max_out_length-1)\n",
        "\n",
        "    # retrieve corresponding embeddings, we do teacher forcing so we use the expected output as next input to our decoder, this helps to learn faster\n",
        "    out_char_embeddings = self.char_embeddings(out_paragraphs) # Shape: (batch_size, max_out_length-1, embedding size)\n",
        "\n",
        "\n",
        "    out_lengths = (out_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "\n",
        "    # Pack embeddings in order to feed them to lstm\n",
        "    out_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=out_char_embeddings,\n",
        "                                                                  lengths=out_lengths.cpu(),\n",
        "                                                                  batch_first=True,\n",
        "                                                                  enforce_sorted=False) # Enables the LSTM to ignore padding elements.\n",
        "\n",
        "    # Pass all embeddings to decoder, we dont need to retrieve hidden and cell states\n",
        "    outputs, (_, _) = self.decoder_lstm(out_char_embeddings, decoder_init_states) # outputs shape: (batch_size, max_out_length-1, lstm_hidden_size)\n",
        "\n",
        "    # Process again the packed embeddings to retrieve an actual torch tensor\n",
        "    outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    # retrieve probability distributions for each decoder's output\n",
        "    logits = self.distribution_nn(outputs) # Shape: (batch_size, max_out_length-1, vocabulary_size)\n",
        "\n",
        "    #concatenate first logits and the rest of the sequences logits\n",
        "    logits = torch.cat((first_logits, logits), dim=1) # Shape: (batch_size, max_out_length, vocabulary_size)\n",
        "\n",
        "    return logits\n",
        "    #################\n",
        "\n",
        "  # Prediction time: This function generates a text up to 'max_predicted_char' character long for each paragraph in the batch.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def predictionStrings(self, in_paragraphs, max_predicted_char=1000) -> List[str]:\n",
        "    ids = []\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "    decoder_init_states = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "\n",
        "    # Decode 'decoder_init_states' into a matrix a character ids (on line per input paragraph in the batch) and then convert it to strings of actual characters.\n",
        "    # You will need to implement a loop at some point.\n",
        "    # To work with probability distributions, you may use \"torch.distributions.Categorical\", but not necessarily.\n",
        "    #################\n",
        "\n",
        "    # we only take the last layer from hidden states, hence -1\n",
        "    last_logits = self.distribution_nn(decoder_init_states[0][-1,:,:])\n",
        "\n",
        "    # Retrieve index where the probability is the highest for each probability distribution\n",
        "    input_tokens = torch.argmax(last_logits, dim=-1)\n",
        "\n",
        "    # Process sequence one by one\n",
        "    for batch_index in range(batch_size):\n",
        "        # Token, hidden and cell states that will be the first inputs fed to the decoder\n",
        "        last_token, last_hidden_state, last_cell_state = input_tokens[batch_index], decoder_init_states[0][:,batch_index,:], decoder_init_states[1][:,batch_index,:]\n",
        "\n",
        "        # Already put first token of the decoded sequence\n",
        "        predicted = [last_token.item()]\n",
        "\n",
        "        # We stop generating tokens when we generate EOP token or that output length exceeds max_predicted_char\n",
        "        while len(predicted)<max_predicted_char and last_token.item()!=EOP_id:\n",
        "\n",
        "            # Decode next token\n",
        "            output, (last_hidden_state, last_cell_state) = self.decoder_lstm(self.char_embeddings(last_token).view(1,-1).contiguous(),\n",
        "                                                                             (last_hidden_state.contiguous(), last_cell_state.contiguous())) # Shape: output (lstm_hidden_size)\n",
        "                                                                                                                                             #        hidden (lstm_layers, lstm_hidden_size)\n",
        "            # Get index of most probable token from probability distribution                                                                                                                                 #        cell   (lstm_layers, lstm_hidden_size)\n",
        "            last_token = torch.argmax(self.distribution_nn(output.data), dim=-1)\n",
        "\n",
        "            # Update generated sequence\n",
        "            predicted.append(last_token.item())\n",
        "        ids.append(predicted)\n",
        "\n",
        "    # turn ids into texts and return them\n",
        "    return ids_to_texts(ids)\n",
        "    #################\n",
        "\n",
        "model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=19, lstm_hidden_size=13, lstm_layers=7, device='cpu')\n",
        "\n",
        "# Tests the training method.\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 0))]).to(model.device) # A batch that contains only one sentence with no padding.\n",
        "# print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "\n",
        "# Tests the prediction methods.\n",
        "batch = batch_generator.get_batch(2, 'dev')\n",
        "model.predictionStrings(batch[0].to(model.device), max_predicted_char=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T4sp6c9Bdch3"
      },
      "outputs": [],
      "source": [
        "# Tests the training method.\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 0))]).to(model.device) # A batch that contains only one sentence with no padding.\n",
        "# print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ftAttD70si-O"
      },
      "outputs": [],
      "source": [
        "# Tests the training method (again).\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 10)), (list(range(10)) + ([batch_generator.padding_idx] * 5))]).to(model.device) # A batch that contains two sentences with some padding (more than necessary).\n",
        "# print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hRPGm_Duq_QY"
      },
      "outputs": [],
      "source": [
        "# Tests the prediction methods.\n",
        "batch = batch_generator.get_batch(2, 'train')\n",
        "model.predictionStrings(batch[0].to(model.device), max_predicted_char=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KIRPPyOCWQ"
      },
      "source": [
        "Training\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zdJSBtNGCX-J"
      },
      "outputs": [],
      "source": [
        "model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=256, lstm_hidden_size=512, lstm_layers=1, device='cuda')\n",
        "\n",
        "import time\n",
        "\n",
        "model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.002\n",
        "momentum = 0.99\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "batch_size = 64 if(not use_toy_dataset) else 8\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length('train') if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 20 if(not use_toy_dataset) else 50\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "time_0 = time.time()\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train() # Tells Pytorch we are in training mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "\n",
        "  batch = batch_generator.get_batch(batch_size, subset=subset, split = 'train')\n",
        "  #print(ids_to_texts(batch[0])); print(ids_to_texts(batch[1]))\n",
        "  in_paragraphs = batch[0].to(model.device)\n",
        "  out_paragraphs = batch[1].to(model.device)\n",
        "\n",
        "  # You have to (i) compute the prediction of the model, (ii) compute the loss, (iii) call \"backward\" on the loss and (iv) store the loss in \"epoch_loss\".\n",
        "  # For the loss, use torch.nn.functional.nll_loss. Computes an average over all tokens of the batch, but do not take into account distribution logits that corresonds to padding characters. Read the documentation and be careful about the shape of your tensors.\n",
        "  ###################\n",
        "  preds = model.trainingLogits(in_paragraphs=in_paragraphs,\n",
        "                               out_paragraphs=out_paragraphs)\n",
        "\n",
        "  loss = torch.nn.functional.nll_loss(preds.transpose(1,2), out_paragraphs, ignore_index=batch_generator.padding_idx)\n",
        "\n",
        "  epoch_loss.append(loss)\n",
        "\n",
        "  loss.backward()\n",
        "  ###################\n",
        "\n",
        "  optimizer.step() # Updates the parameters.\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size):\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "    duration = time.time() - time_0\n",
        "    print(f\"{duration} s elapsed (i.e. {duration / (epoch_id + 1)} s/epoch)\")\n",
        "\n",
        "    # Example of generation\n",
        "    batch = batch_generator.get_batch(1, subset=subset, split = 'dev')\n",
        "    print(ids_to_texts(batch[0])) # Input paragraph\n",
        "    print(model.predictionStrings(batch[0].to(model.device), max_predicted_char=512)) # Generated output paragraph.\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kna3OLXc9_H-"
      },
      "outputs": [],
      "source": [
        "use_toy_dataset = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cxs4UZRAHxx3"
      },
      "outputs": [],
      "source": [
        "if(use_toy_dataset):\n",
        "  prompt = [\"AAAA. Now, please write 3 i.\" + EOP] #[paragraphs[0]]\n",
        "  print(prompt[0])\n",
        "  print(f\"(Is this prompt in the training set? {prompt[0] in paragraphs})\\n\")\n",
        "\n",
        "  for _ in range(10):\n",
        "    batch = batch_generator.turn_into_batch(prompt)\n",
        "    gen_texts = model.predictionStrings(batch.to(model.device), max_predicted_char=128)\n",
        "\n",
        "    print(gen_texts[0])\n",
        "    prompt = [gen_texts[0] + EOP]\n",
        "else:\n",
        "  prompt = [\"Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\"] * 10\n",
        "  batch = batch_generator.turn_into_batch(prompt)\n",
        "  gen_texts = model.predictionStrings(batch.to(model.device), max_predicted_char=1024)\n",
        "\n",
        "  print(prompt[0])\n",
        "  print()\n",
        "  for i, gen_text in enumerate(gen_texts):\n",
        "    print(f\"{i}: \", end=\"\")\n",
        "    print(gen_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50HbcnPmXkMT"
      },
      "source": [
        "If your system does not work as expected, check that you are using a sensible loss function, but also check that your implementation matches the architecture depicted in https://moodle.u-paris.fr/mod/resource/view.php?id=648001.\n",
        "\n",
        "If you cannot get your model to work even on the toy dataset, then there must be a bug somewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2GHjtPr4Mw2"
      },
      "source": [
        "Read the remarks at the beginning of the TP again.\n",
        "\n",
        "Once you are sure that your system is correctly implemented and generates texts that look a little bit like natural language, find ways to improve the system.\n",
        "Here are some ideas (ordered arbitrarily):\n",
        "\n",
        "*   Compute a measure that evaluates the performance of the model.\n",
        "*   Split your dataset into a training and a development section, and use this split in a relevant way.\n",
        "*   Implement beam decoding instead of greedy decoding.\n",
        "*   Use other units of text instead of characters (ex: words, word-pieces).\n",
        "*   Add more data to the dataset.\n",
        "*   Use graphs to visualise the training process and the predictions.\n",
        "\n",
        "Document in a text cell all of the changes that you make to the system and describe their impact (qualitatively **and** quantitatively)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DN7BfhTj-7b"
      },
      "source": [
        "_____________________________\n",
        "- evaluation: ~BLEU~ / **ROUGE?** / combien de mots sont pas de vrais mots (pas dans le dictionnaire) (noter auee BLEU a bien marché pour toy set mais pas pour Flaubert - score > 1.0)\n",
        "- ~split et tuner sur dev (early stopping~ + random search)\n",
        "- beam https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
        "- tokenization en mots ou BPE pour les subwords (HuggingFace comme d'hab)\n",
        "- ajouter qqch d'autres de Flaubert je sais pas\n",
        "- il y a un todo dans le batch generator - fin de chapitre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghethT3lCseU"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aKa8b0Q0XdmR"
      },
      "outputs": [],
      "source": [
        "!pip install -q sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yhAtWf2pkwZm"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "\n",
        "bleu = BLEU(tokenize='char')\n",
        "\n",
        "def evaluate(model, eval_batch_size, split, subset):\n",
        "\n",
        "  if split == 'train':\n",
        "    paragraphs = batch_generator.train_paragraphs\n",
        "  elif split == 'dev':\n",
        "    paragraphs = batch_generator.dev_paragraphs\n",
        "\n",
        "  predicted = []\n",
        "  golden = []\n",
        "\n",
        "  for batch in batch_generator.all_batches(eval_batch_size, split=split, subset=subset):\n",
        "\n",
        "    predicted = model.predictionStrings(batch[0].to(model.device))\n",
        "    sentences = [x.tolist() for i, x in enumerate(batch[1])]\n",
        "    golden = [ids_to_texts([sent]) for sent in sentences]\n",
        "  print(len(batch[0], len(batch[1])))\n",
        "  print(len(predicted),predicted)\n",
        "  print(len(golden),golden)\n",
        "\n",
        "  return bleu.corpus_score(predicted, golden).score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KpKwuLTqERIF"
      },
      "outputs": [],
      "source": [
        "# test the evaluate method\n",
        "score = evaluate(model, 64, split = 'train', subset=None)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_ZloUIT9stL"
      },
      "source": [
        "# Wrapping everything into suitable form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gxI4-0zs9sUR"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "def trainer(nb_epoch=10, learning_rate=0.002,\n",
        "            momentum=0.99, l2_reg=0.0001, batch_size=16,\n",
        "            early_stopping=True,\n",
        "            plot=True,\n",
        "            device = 'cuda'\n",
        "            ):\n",
        "\n",
        "  model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=128, lstm_hidden_size=256, lstm_layers=3, device=device)\n",
        "\n",
        "\n",
        "  model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "  subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "  epoch_size = batch_generator.length(split='train') if(subset is None) else subset # In number of instances\n",
        "\n",
        "\n",
        "# ----------------------------------- # variables used for early stopping # ----------------------------------- #\n",
        "\n",
        "  patience = 4                         # how many epochs without improvement we are ready to tolerate\n",
        "  nb_epochs_no_improvement = 0         # how many consequent epochs without improvement there really are\n",
        "  best_loss = 10000                    # beat accuracy achieved (nedded to define if there is an improvement)\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "  epoch_id = 0 # Id of the current epoch\n",
        "  instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "  time_0 = time.time()\n",
        "\n",
        "  epoch_losses = []\n",
        "  train_losses = []\n",
        "  train_scores = []\n",
        "  dev_scores = []\n",
        "  # dev_losses = []     # or accuracies if we find a way to calculate it\n",
        "\n",
        "  while(epoch_id < nb_epoch):\n",
        "    model.train() # Tells Pytorch we are in training mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "    model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "\n",
        "    batch = batch_generator.get_batch(batch_size, subset=subset, split = 'train')\n",
        "    in_paragraphs = batch[0].to(model.device)\n",
        "    out_paragraphs = batch[1].to(model.device)\n",
        "\n",
        "    preds = model.trainingLogits(in_paragraphs=in_paragraphs,\n",
        "                                out_paragraphs=out_paragraphs)\n",
        "\n",
        "    loss = torch.nn.functional.nll_loss(preds.transpose(1,2), out_paragraphs, ignore_index=batch_generator.padding_idx)\n",
        "\n",
        "    epoch_losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step() # Updates the parameters.\n",
        "\n",
        "    instances_processed += batch_size\n",
        "    if(instances_processed > epoch_size):\n",
        "      print(f\"\\n ---- END OF EPOCH {epoch_id} ----\")\n",
        "      print(f\"Average loss: {sum(epoch_losses) / len(epoch_losses)}.\")\n",
        "      train_losses.append(sum(epoch_losses) / len(epoch_losses))\n",
        "\n",
        "  # -------------------------------------------------------- evaluation -------------------------------------------------------- #\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        score_train = evaluate(model, eval_batch_size = 128, split = \"train\", subset = 10)\n",
        "        print(f\"Score on the train set: {score_train}.\")\n",
        "        train_scores.append(score_train)\n",
        "\n",
        "        score_dev = evaluate(model, 128, split = \"dev\", subset = 10)\n",
        "        print(f\"Score on the dev set: {score_dev}.\")\n",
        "        dev_scores.append(score_dev)\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "      # Example of generation\n",
        "      print(' * example of the output * ')\n",
        "      batch = batch_generator.get_batch(1, subset=subset, split='train')\n",
        "      print(ids_to_texts(batch[0])) # Input paragraph\n",
        "      print(model.predictionStrings(batch[0].to(model.device), max_predicted_char=16)) # Generated output paragraph.\n",
        "\n",
        "  # --------------------------------------------- # early stopping implementation # -------------------------------------------- #\n",
        "\n",
        "      if early_stopping:\n",
        "\n",
        "\n",
        "        if loss < best_loss:                                   # if the current loss is the new best one ->\n",
        "          best_loss = loss                                     # update the best loss and set the counter back to 0\n",
        "          nb_epochs_no_improvement = 0\n",
        "        else:\n",
        "          nb_epochs_no_improvement += 1                                        # if there is no loss improvement -> count this epoch\n",
        "          if nb_epochs_no_improvement == patience:                             # if the patience is reached -> stop the training\n",
        "            print(f'Early stopping at epoch {epoch_id}')\n",
        "            print(f\"Smallest loss achieved is {best_loss}\")\n",
        "            nb_epochs_total = epoch_id\n",
        "            break\n",
        "\n",
        "  # ----------------------------------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "      epoch_id += 1\n",
        "      instances_processed -= epoch_size\n",
        "      epoch_loss = []\n",
        "\n",
        "\n",
        "  duration = time.time() / 1000.0 #convert ms to s\n",
        "  print(f\"\\n This config took {duration} to train \\n\")\n",
        "\n",
        "  # ---------------------------- # plotting the accuracies on the dev an train set during training # ---------------------------- #\n",
        "\n",
        "\n",
        "  if plot:\n",
        "\n",
        "    epochs = [e + 1 for e in range(nb_epochs_total + 1)] if early_stopping else [e+1 for e in range(nb_epoch)]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2)\n",
        "    print(train_losses)\n",
        "    ax1.plot(epochs, train_losses, label='Train Losses', marker='o')\n",
        "    ax2.plot(epochs, train_scores, label='Train Scores', marker='o')\n",
        "    ax2.plot(epochs, dev_scores, label='Dev Scores', marker='o')\n",
        "\n",
        "    ax1.set(ylabel = 'Loss', xticks = epochs)\n",
        "    ax2.set(xlabel = 'Epochs', ylabel = 'BLEU score', xticks = epochs)\n",
        "\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A82gYwVe6jLI"
      },
      "outputs": [],
      "source": [
        "# used for toy corpus\n",
        "\n",
        "#  def trainer(nb_epoch=10, learning_rate=0.002,\n",
        "#             momentum=0.99, l2_reg=0.0001, batch_size=16,\n",
        "#             early_stopping=True,\n",
        "#             plot=True,\n",
        "#             device = 'cuda'\n",
        "#             ):\n",
        "\n",
        "#   model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=128, lstm_hidden_size=256, lstm_layers=3, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-mU6pKovBKdI"
      },
      "outputs": [],
      "source": [
        "trainer(nb_epoch=20, batch_size=16, early_stopping=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZgKHfkBwh2LA"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}